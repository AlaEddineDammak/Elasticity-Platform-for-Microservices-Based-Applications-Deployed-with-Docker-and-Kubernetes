* 
* ==> Audit <==
* |--------------|-----------------------|----------|------------|---------|----------------------|----------------------|
|   Command    |         Args          | Profile  |    User    | Version |      Start Time      |       End Time       |
|--------------|-----------------------|----------|------------|---------|----------------------|----------------------|
| start        |                       | minikube | aladin_257 | v1.32.0 | 28 Feb 24 16:55 CET  | 28 Feb 24 16:55 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 17:17 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 17:17 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 17:17 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 17:21 CET  |                      |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 17:21 CET  | 01 Mar 24 17:21 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 18:34 CET  | 01 Mar 24 18:34 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 18:34 CET  | 01 Mar 24 18:39 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 18:39 CET  | 01 Mar 24 18:39 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 19:39 CET  | 01 Mar 24 19:39 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:00 CET  | 01 Mar 24 20:01 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:06 CET  | 01 Mar 24 20:06 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:12 CET  | 01 Mar 24 20:12 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:22 CET  | 01 Mar 24 20:22 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:24 CET  | 01 Mar 24 20:25 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:33 CET  | 01 Mar 24 20:33 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:33 CET  | 01 Mar 24 20:34 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 01 Mar 24 20:37 CET  | 01 Mar 24 20:37 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 14:11 CET  | 03 Mar 24 14:11 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 14:12 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 14:14 CET  | 03 Mar 24 14:15 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 14:15 CET  | 03 Mar 24 14:15 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 14:15 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 15:38 CET  | 03 Mar 24 15:38 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 15:40 CET  | 03 Mar 24 15:41 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 15:41 CET  | 03 Mar 24 15:42 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 15:44 CET  | 03 Mar 24 15:45 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 03 Mar 24 17:32 CET  | 03 Mar 24 17:32 CET  |
| ip           |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 18:54 CET  | 05 Mar 24 18:54 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:17 CET  | 05 Mar 24 19:18 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:19 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:20 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:20 CET  | 05 Mar 24 19:20 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:21 CET  | 05 Mar 24 19:21 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:21 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:23 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:23 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:26 CET  | 05 Mar 24 19:27 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:30 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:31 CET  | 05 Mar 24 19:32 CET  |
| addons       | enable metrics-server | minikube | aladin_257 | v1.32.0 | 05 Mar 24 19:32 CET  | 05 Mar 24 19:32 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 20 Mar 24 15:11 CET  |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 20 Mar 24 15:12 CET  | 20 Mar 24 15:13 CET  |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 20 Mar 24 16:11 CET  | 20 Mar 24 16:11 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 28 Mar 24 10:05 CET  | 28 Mar 24 10:07 CET  |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 28 Mar 24 10:07 CET  | 28 Mar 24 10:07 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 28 Mar 24 10:07 CET  | 28 Mar 24 10:08 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 29 Mar 24 10:07 CET  | 29 Mar 24 10:08 CET  |
| start        |                       | minikube | aladin_257 | v1.32.0 | 13 Apr 24 16:03 CEST |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 13 Apr 24 16:04 CEST | 13 Apr 24 16:04 CEST |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 13 Apr 24 18:48 CEST | 13 Apr 24 18:48 CEST |
| stop         |                       | minikube | aladin_257 | v1.32.0 | 15 Apr 24 18:26 CEST | 15 Apr 24 18:27 CEST |
| start        |                       | minikube | aladin_257 | v1.32.0 | 15 Apr 24 18:27 CEST |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 15 Apr 24 18:27 CEST | 15 Apr 24 18:28 CEST |
| start        |                       | minikube | aladin_257 | v1.32.0 | 15 Apr 24 19:04 CEST |                      |
| start        |                       | minikube | aladin_257 | v1.32.0 | 15 Apr 24 19:04 CEST | 15 Apr 24 19:05 CEST |
| addons       | enable metrics-server | minikube | aladin_257 | v1.32.0 | 15 Apr 24 20:02 CEST | 15 Apr 24 20:02 CEST |
| update-check |                       | minikube | aladin_257 | v1.32.0 | 17 Apr 24 12:46 CEST |                      |
| service      | frontend              | minikube | aladin_257 | v1.32.0 | 17 Apr 24 13:17 CEST | 17 Apr 24 13:18 CEST |
| addons       | enable yakd           | minikube | aladin_257 | v1.32.0 | 17 Apr 24 19:39 CEST |                      |
|--------------|-----------------------|----------|------------|---------|----------------------|----------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/04/15 19:04:40
Running on machine: LAPTOP-GO98DRBB
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0415 19:04:40.771438    1546 out.go:296] Setting OutFile to fd 1 ...
I0415 19:04:40.771762    1546 out.go:348] isatty.IsTerminal(1) = true
I0415 19:04:40.771766    1546 out.go:309] Setting ErrFile to fd 2...
I0415 19:04:40.771769    1546 out.go:348] isatty.IsTerminal(2) = true
I0415 19:04:40.771875    1546 root.go:338] Updating PATH: /home/aladin_257/.minikube/bin
I0415 19:04:40.772181    1546 out.go:303] Setting JSON to false
I0415 19:04:40.772750    1546 start.go:128] hostinfo: {"hostname":"LAPTOP-GO98DRBB","uptime":77,"bootTime":1713200604,"procs":32,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.133.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"f28969dd-f077-4495-b1f5-475bb1c1c9da"}
I0415 19:04:40.772794    1546 start.go:138] virtualization:  guest
I0415 19:04:40.775460    1546 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04 (amd64)
I0415 19:04:40.777353    1546 notify.go:220] Checking for updates...
I0415 19:04:40.777574    1546 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0415 19:04:40.777631    1546 driver.go:378] Setting default libvirt URI to qemu:///system
I0415 19:04:40.841359    1546 docker.go:122] docker version: linux-24.0.6:Docker Desktop
I0415 19:04:40.841673    1546 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0415 19:04:40.970316    1546 info.go:266] docker info: {ID:fcdee6ae-2970-41dd-93c5-ac32d790d612 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:66 SystemTime:2024-04-15 17:04:40.958361515 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7196532736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:LAPTOP-GO98DRBB Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I0415 19:04:40.970430    1546 docker.go:295] overlay module found
I0415 19:04:40.972895    1546 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0415 19:04:40.974480    1546 start.go:298] selected driver: docker
I0415 19:04:40.974486    1546 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aladin_257:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0415 19:04:40.974544    1546 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0415 19:04:40.974618    1546 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0415 19:04:41.092548    1546 info.go:266] docker info: {ID:fcdee6ae-2970-41dd-93c5-ac32d790d612 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:66 SystemTime:2024-04-15 17:04:41.080892959 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7196532736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:LAPTOP-GO98DRBB Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I0415 19:04:41.093138    1546 cni.go:84] Creating CNI manager for ""
I0415 19:04:41.093149    1546 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0415 19:04:41.093156    1546 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aladin_257:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0415 19:04:41.095412    1546 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0415 19:04:41.097255    1546 cache.go:121] Beginning downloading kic base image for docker with docker
I0415 19:04:41.098824    1546 out.go:177] üöú  Pulling base image ...
I0415 19:04:41.100408    1546 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0415 19:04:41.100436    1546 preload.go:148] Found local preload: /home/aladin_257/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0415 19:04:41.100441    1546 cache.go:56] Caching tarball of preloaded images
I0415 19:04:41.100510    1546 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42 in local docker daemon
I0415 19:04:41.100533    1546 preload.go:174] Found /home/aladin_257/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0415 19:04:41.100540    1546 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0415 19:04:41.100625    1546 profile.go:148] Saving config to /home/aladin_257/.minikube/profiles/minikube/config.json ...
I0415 19:04:41.157358    1546 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42 in local docker daemon, skipping pull
I0415 19:04:41.157373    1546 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42 exists in daemon, skipping load
I0415 19:04:41.157397    1546 cache.go:194] Successfully downloaded all kic artifacts
I0415 19:04:41.157432    1546 start.go:365] acquiring machines lock for minikube: {Name:mkb936d028dc52bd590b90150dd1eb546fd1a3c4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0415 19:04:41.157512    1546 start.go:369] acquired machines lock for "minikube" in 63.37¬µs
I0415 19:04:41.157524    1546 start.go:96] Skipping create...Using existing machine configuration
I0415 19:04:41.157529    1546 fix.go:54] fixHost starting: 
I0415 19:04:41.157705    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:04:41.210103    1546 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0415 19:04:41.210119    1546 fix.go:128] unexpected machine state, will restart: <nil>
I0415 19:04:41.212610    1546 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0415 19:04:41.214288    1546 machine.go:88] provisioning docker machine ...
I0415 19:04:41.214310    1546 ubuntu.go:169] provisioning hostname "minikube"
I0415 19:04:41.214367    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:41.268680    1546 main.go:141] libmachine: Using SSH client type: native
I0415 19:04:41.269088    1546 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 51537 <nil> <nil>}
I0415 19:04:41.269099    1546 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0415 19:04:41.393961    1546 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0415 19:04:41.394032    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:41.451761    1546 main.go:141] libmachine: Using SSH client type: native
I0415 19:04:41.452008    1546 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 51537 <nil> <nil>}
I0415 19:04:41.452016    1546 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0415 19:04:41.573260    1546 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0415 19:04:41.573282    1546 ubuntu.go:175] set auth options {CertDir:/home/aladin_257/.minikube CaCertPath:/home/aladin_257/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aladin_257/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aladin_257/.minikube/machines/server.pem ServerKeyPath:/home/aladin_257/.minikube/machines/server-key.pem ClientKeyPath:/home/aladin_257/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aladin_257/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aladin_257/.minikube}
I0415 19:04:41.573322    1546 ubuntu.go:177] setting up certificates
I0415 19:04:41.573333    1546 provision.go:83] configureAuth start
I0415 19:04:41.573396    1546 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0415 19:04:41.628275    1546 provision.go:138] copyHostCerts
I0415 19:04:41.628318    1546 exec_runner.go:144] found /home/aladin_257/.minikube/key.pem, removing ...
I0415 19:04:41.628334    1546 exec_runner.go:203] rm: /home/aladin_257/.minikube/key.pem
I0415 19:04:41.628419    1546 exec_runner.go:151] cp: /home/aladin_257/.minikube/certs/key.pem --> /home/aladin_257/.minikube/key.pem (1675 bytes)
I0415 19:04:41.628518    1546 exec_runner.go:144] found /home/aladin_257/.minikube/ca.pem, removing ...
I0415 19:04:41.628521    1546 exec_runner.go:203] rm: /home/aladin_257/.minikube/ca.pem
I0415 19:04:41.628552    1546 exec_runner.go:151] cp: /home/aladin_257/.minikube/certs/ca.pem --> /home/aladin_257/.minikube/ca.pem (1090 bytes)
I0415 19:04:41.628601    1546 exec_runner.go:144] found /home/aladin_257/.minikube/cert.pem, removing ...
I0415 19:04:41.628613    1546 exec_runner.go:203] rm: /home/aladin_257/.minikube/cert.pem
I0415 19:04:41.628642    1546 exec_runner.go:151] cp: /home/aladin_257/.minikube/certs/cert.pem --> /home/aladin_257/.minikube/cert.pem (1131 bytes)
I0415 19:04:41.628744    1546 provision.go:112] generating server cert: /home/aladin_257/.minikube/machines/server.pem ca-key=/home/aladin_257/.minikube/certs/ca.pem private-key=/home/aladin_257/.minikube/certs/ca-key.pem org=aladin_257.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0415 19:04:41.747741    1546 provision.go:172] copyRemoteCerts
I0415 19:04:41.748058    1546 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0415 19:04:41.748173    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:41.819727    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:04:41.907199    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0415 19:04:41.924317    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I0415 19:04:41.942631    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0415 19:04:41.959886    1546 provision.go:86] duration metric: configureAuth took 386.540885ms
I0415 19:04:41.959902    1546 ubuntu.go:193] setting minikube options for container-runtime
I0415 19:04:41.960042    1546 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0415 19:04:41.960087    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.014973    1546 main.go:141] libmachine: Using SSH client type: native
I0415 19:04:42.015224    1546 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 51537 <nil> <nil>}
I0415 19:04:42.015229    1546 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0415 19:04:42.133818    1546 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0415 19:04:42.133830    1546 ubuntu.go:71] root file system type: overlay
I0415 19:04:42.133938    1546 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0415 19:04:42.134003    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.202881    1546 main.go:141] libmachine: Using SSH client type: native
I0415 19:04:42.203253    1546 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 51537 <nil> <nil>}
I0415 19:04:42.203331    1546 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0415 19:04:42.342428    1546 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0415 19:04:42.342496    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.396038    1546 main.go:141] libmachine: Using SSH client type: native
I0415 19:04:42.396321    1546 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 51537 <nil> <nil>}
I0415 19:04:42.396333    1546 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0415 19:04:42.511328    1546 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0415 19:04:42.511348    1546 machine.go:91] provisioned docker machine in 1.297251576s
I0415 19:04:42.511363    1546 start.go:300] post-start starting for "minikube" (driver="docker")
I0415 19:04:42.511375    1546 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0415 19:04:42.511455    1546 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0415 19:04:42.511541    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.582959    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:04:42.670845    1546 ssh_runner.go:195] Run: cat /etc/os-release
I0415 19:04:42.675791    1546 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0415 19:04:42.675827    1546 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0415 19:04:42.675833    1546 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0415 19:04:42.675843    1546 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0415 19:04:42.675864    1546 filesync.go:126] Scanning /home/aladin_257/.minikube/addons for local assets ...
I0415 19:04:42.675950    1546 filesync.go:126] Scanning /home/aladin_257/.minikube/files for local assets ...
I0415 19:04:42.675971    1546 start.go:303] post-start completed in 164.601535ms
I0415 19:04:42.676020    1546 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0415 19:04:42.676059    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.740569    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:04:42.834082    1546 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0415 19:04:42.838085    1546 fix.go:56] fixHost completed within 1.680749825s
I0415 19:04:42.838109    1546 start.go:83] releasing machines lock for "minikube", held for 1.680791495s
I0415 19:04:42.838183    1546 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0415 19:04:42.913372    1546 ssh_runner.go:195] Run: cat /version.json
I0415 19:04:42.913419    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:42.913436    1546 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0415 19:04:42.913472    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:04:43.010293    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:04:43.013133    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:04:44.376153    1546 ssh_runner.go:235] Completed: cat /version.json: (1.462763182s)
I0415 19:04:44.376153    1546 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.462691195s)
I0415 19:04:44.376325    1546 ssh_runner.go:195] Run: systemctl --version
I0415 19:04:44.381414    1546 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0415 19:04:44.385590    1546 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0415 19:04:44.405582    1546 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0415 19:04:44.405792    1546 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0415 19:04:44.414276    1546 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0415 19:04:44.414304    1546 start.go:472] detecting cgroup driver to use...
I0415 19:04:44.414338    1546 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0415 19:04:44.414438    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0415 19:04:44.432005    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0415 19:04:44.439974    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0415 19:04:44.447915    1546 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0415 19:04:44.447992    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0415 19:04:44.456974    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0415 19:04:44.465229    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0415 19:04:44.473041    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0415 19:04:44.481402    1546 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0415 19:04:44.488182    1546 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0415 19:04:44.495925    1546 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0415 19:04:44.502193    1546 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0415 19:04:44.508576    1546 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0415 19:04:44.594837    1546 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0415 19:04:44.697015    1546 start.go:472] detecting cgroup driver to use...
I0415 19:04:44.697070    1546 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0415 19:04:44.697157    1546 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0415 19:04:44.708370    1546 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0415 19:04:44.708426    1546 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0415 19:04:44.718891    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0415 19:04:44.733119    1546 ssh_runner.go:195] Run: which cri-dockerd
I0415 19:04:44.735674    1546 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0415 19:04:44.745266    1546 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0415 19:04:44.760222    1546 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0415 19:04:44.861698    1546 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0415 19:04:44.966907    1546 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0415 19:04:44.967004    1546 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0415 19:04:44.979837    1546 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0415 19:04:45.065793    1546 ssh_runner.go:195] Run: sudo systemctl restart docker
I0415 19:04:45.377788    1546 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0415 19:04:45.472046    1546 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0415 19:04:45.577746    1546 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0415 19:04:45.675164    1546 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0415 19:04:45.784906    1546 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0415 19:04:45.798988    1546 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0415 19:04:45.891112    1546 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0415 19:04:45.973055    1546 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0415 19:04:45.973139    1546 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0415 19:04:45.976567    1546 start.go:540] Will wait 60s for crictl version
I0415 19:04:45.976622    1546 ssh_runner.go:195] Run: which crictl
I0415 19:04:45.979613    1546 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0415 19:04:46.021320    1546 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0415 19:04:46.021383    1546 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0415 19:04:46.043847    1546 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0415 19:04:46.070576    1546 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0415 19:04:46.070776    1546 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0415 19:04:46.146646    1546 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0415 19:04:46.151714    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0415 19:04:46.222142    1546 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0415 19:04:46.222198    1546 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0415 19:04:46.241947    1546 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
us-docker.pkg.dev/google-samples/containers/gke/gb-redis-follower:v2
registry.k8s.io/pause:3.9
us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
kubernetesui/dashboard:v2.7.0
kubernetesui/metrics-scraper:v1.0.8
gcr.io/k8s-minikube/storage-provisioner:v5
redis:6.0.5

-- /stdout --
I0415 19:04:46.241959    1546 docker.go:601] Images already preloaded, skipping extraction
I0415 19:04:46.242013    1546 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0415 19:04:46.262301    1546 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
us-docker.pkg.dev/google-samples/containers/gke/gb-redis-follower:v2
registry.k8s.io/pause:3.9
us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
kubernetesui/dashboard:v2.7.0
kubernetesui/metrics-scraper:v1.0.8
gcr.io/k8s-minikube/storage-provisioner:v5
redis:6.0.5

-- /stdout --
I0415 19:04:46.262325    1546 cache_images.go:84] Images are preloaded, skipping loading
I0415 19:04:46.262415    1546 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0415 19:04:46.439863    1546 cni.go:84] Creating CNI manager for ""
I0415 19:04:46.439875    1546 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0415 19:04:46.440631    1546 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0415 19:04:46.440654    1546 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0415 19:04:46.440746    1546 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0415 19:04:46.440807    1546 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0415 19:04:46.440854    1546 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0415 19:04:46.449021    1546 binaries.go:44] Found k8s binaries, skipping transfer
I0415 19:04:46.449071    1546 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0415 19:04:46.455307    1546 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0415 19:04:46.468582    1546 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0415 19:04:46.482430    1546 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0415 19:04:46.497168    1546 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0415 19:04:46.500240    1546 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0415 19:04:46.508268    1546 certs.go:56] Setting up /home/aladin_257/.minikube/profiles/minikube for IP: 192.168.49.2
I0415 19:04:46.508285    1546 certs.go:190] acquiring lock for shared ca certs: {Name:mk29b0d08fa0357466764fadface0f845bc6edda Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0415 19:04:46.508852    1546 certs.go:199] skipping minikubeCA CA generation: /home/aladin_257/.minikube/ca.key
I0415 19:04:46.509357    1546 certs.go:199] skipping proxyClientCA CA generation: /home/aladin_257/.minikube/proxy-client-ca.key
I0415 19:04:46.509913    1546 certs.go:315] skipping minikube-user signed cert generation: /home/aladin_257/.minikube/profiles/minikube/client.key
I0415 19:04:46.510369    1546 certs.go:315] skipping minikube signed cert generation: /home/aladin_257/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0415 19:04:46.510694    1546 certs.go:315] skipping aggregator signed cert generation: /home/aladin_257/.minikube/profiles/minikube/proxy-client.key
I0415 19:04:46.510778    1546 certs.go:437] found cert: /home/aladin_257/.minikube/certs/home/aladin_257/.minikube/certs/ca-key.pem (1679 bytes)
I0415 19:04:46.510798    1546 certs.go:437] found cert: /home/aladin_257/.minikube/certs/home/aladin_257/.minikube/certs/ca.pem (1090 bytes)
I0415 19:04:46.510825    1546 certs.go:437] found cert: /home/aladin_257/.minikube/certs/home/aladin_257/.minikube/certs/cert.pem (1131 bytes)
I0415 19:04:46.510842    1546 certs.go:437] found cert: /home/aladin_257/.minikube/certs/home/aladin_257/.minikube/certs/key.pem (1675 bytes)
I0415 19:04:46.513012    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0415 19:04:46.532065    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0415 19:04:46.549629    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0415 19:04:46.567247    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0415 19:04:46.584679    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0415 19:04:46.604184    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0415 19:04:46.622902    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0415 19:04:46.640320    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0415 19:04:46.657959    1546 ssh_runner.go:362] scp /home/aladin_257/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0415 19:04:46.675314    1546 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0415 19:04:46.690116    1546 ssh_runner.go:195] Run: openssl version
I0415 19:04:46.698714    1546 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0415 19:04:46.707236    1546 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0415 19:04:46.710409    1546 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan 28 22:29 /usr/share/ca-certificates/minikubeCA.pem
I0415 19:04:46.710439    1546 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0415 19:04:46.716403    1546 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0415 19:04:46.724000    1546 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0415 19:04:46.727310    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0415 19:04:46.735447    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0415 19:04:46.742753    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0415 19:04:46.749448    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0415 19:04:46.754992    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0415 19:04:46.760605    1546 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0415 19:04:46.766161    1546 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aladin_257:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0415 19:04:46.766247    1546 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0415 19:04:46.783179    1546 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0415 19:04:46.790808    1546 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0415 19:04:46.790823    1546 kubeadm.go:636] restartCluster start
I0415 19:04:46.790879    1546 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0415 19:04:46.797613    1546 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0415 19:04:46.797681    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0415 19:04:46.856059    1546 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:49387"
I0415 19:04:46.856075    1546 kubeconfig.go:135] verify returned: got: 127.0.0.1:49387, want: 127.0.0.1:51541
I0415 19:04:46.856413    1546 lock.go:35] WriteFile acquiring /home/aladin_257/.kube/config: {Name:mk70a5a6896bb31776d2729d95acb49dad88a16e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0415 19:04:46.865490    1546 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0415 19:04:46.876827    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:46.876873    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:46.889664    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:46.889676    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:46.889721    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:46.897712    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:47.398596    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:47.398750    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:47.410652    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:47.898383    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:47.898463    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:47.909127    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:48.398783    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:48.398875    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:48.407892    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:48.898616    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:48.898708    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:48.909227    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:49.398820    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:49.398924    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:49.409818    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:49.898797    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:49.898873    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:49.911153    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:50.398072    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:50.398181    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:50.407588    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:50.897861    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:50.897931    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:50.910817    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:51.398235    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:51.398321    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:51.409341    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:51.898472    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:51.898556    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:51.909428    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:52.398742    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:52.398838    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:52.407550    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:52.897948    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:52.898160    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:52.910582    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:53.397807    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:53.397890    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:53.406280    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:53.898661    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:53.898759    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:53.909393    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:54.398119    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:54.398205    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:54.409434    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:54.898282    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:54.898350    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:54.907978    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:55.398547    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:55.398646    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:55.407935    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:55.898796    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:55.898896    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:55.908541    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:56.398611    1546 api_server.go:166] Checking apiserver status ...
I0415 19:04:56.398828    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0415 19:04:56.417941    1546 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0415 19:04:56.877275    1546 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0415 19:04:56.877311    1546 kubeadm.go:1128] stopping kube-system containers ...
I0415 19:04:56.877401    1546 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0415 19:04:56.907149    1546 docker.go:469] Stopping containers: [bab695339089 de8e38428a2f 9f2039bd6273 9076aa24270b 669400216a1e 2ad4cbaa5918 45c45dcf326c 66bd0b830153 41f98e79bd94 f11c9c7d3e23 a33ec903e708 4c80463254fd a3d14ef9360b b6dbd30804d9 67af5c812857 4f5edf02838e 5daff855ef22 7f443a5a654c b92c0c4e42c3 a34515896e9f 1a5c5e2bfc43 1fcde4ff0348 3b52bd909b48 b5b590987c74 48428cbf4d38 f1c3d88c9f2d d89bd56849ee 2bc2cf43d77d 87ce59db4c55 8718f68e4640 fc0029fb69fd]
I0415 19:04:56.907231    1546 ssh_runner.go:195] Run: docker stop bab695339089 de8e38428a2f 9f2039bd6273 9076aa24270b 669400216a1e 2ad4cbaa5918 45c45dcf326c 66bd0b830153 41f98e79bd94 f11c9c7d3e23 a33ec903e708 4c80463254fd a3d14ef9360b b6dbd30804d9 67af5c812857 4f5edf02838e 5daff855ef22 7f443a5a654c b92c0c4e42c3 a34515896e9f 1a5c5e2bfc43 1fcde4ff0348 3b52bd909b48 b5b590987c74 48428cbf4d38 f1c3d88c9f2d d89bd56849ee 2bc2cf43d77d 87ce59db4c55 8718f68e4640 fc0029fb69fd
I0415 19:04:56.930659    1546 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0415 19:04:56.941069    1546 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0415 19:04:56.947646    1546 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Mar  1 17:35 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Apr 15 16:28 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar  1 17:38 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Apr 15 16:28 /etc/kubernetes/scheduler.conf

I0415 19:04:56.947703    1546 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0415 19:04:56.955619    1546 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0415 19:04:56.963628    1546 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0415 19:04:56.970753    1546 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0415 19:04:56.970835    1546 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0415 19:04:56.977757    1546 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0415 19:04:56.986702    1546 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0415 19:04:56.986798    1546 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0415 19:04:56.993937    1546 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0415 19:04:57.001755    1546 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0415 19:04:57.001769    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:04:57.176045    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:04:58.209806    1546 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.033739961s)
I0415 19:04:58.209823    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:04:58.369925    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:04:58.434743    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:04:58.516936    1546 api_server.go:52] waiting for apiserver process to appear ...
I0415 19:04:58.517053    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:04:58.530151    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:04:59.044333    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:04:59.545299    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:05:00.046812    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:05:00.544398    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:05:01.045058    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:05:01.060030    1546 api_server.go:72] duration metric: took 2.543083908s to wait for apiserver process to appear ...
I0415 19:05:01.060070    1546 api_server.go:88] waiting for apiserver healthz status ...
I0415 19:05:01.060111    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:01.062193    1546 api_server.go:269] stopped: https://127.0.0.1:51541/healthz: Get "https://127.0.0.1:51541/healthz": EOF
I0415 19:05:01.062240    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:01.064146    1546 api_server.go:269] stopped: https://127.0.0.1:51541/healthz: Get "https://127.0.0.1:51541/healthz": EOF
I0415 19:05:01.564528    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:01.566230    1546 api_server.go:269] stopped: https://127.0.0.1:51541/healthz: Get "https://127.0.0.1:51541/healthz": EOF
I0415 19:05:02.064361    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:04.440622    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0415 19:05:04.440648    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0415 19:05:04.440664    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:04.542251    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:04.542283    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:04.564342    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:04.635155    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:04.635195    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:05.064532    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:05.069660    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:05.069682    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:05.565120    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:05.642299    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:05.642345    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:06.064782    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:06.141738    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:06.141771    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:06.564736    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:06.637090    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:06.637143    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:07.064411    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:07.136776    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:07.136816    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:07.564825    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:07.631691    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0415 19:05:07.631840    1546 api_server.go:103] status: https://127.0.0.1:51541/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0415 19:05:08.064965    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:08.136219    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 200:
ok
I0415 19:05:08.149611    1546 api_server.go:141] control plane version: v1.28.3
I0415 19:05:08.149647    1546 api_server.go:131] duration metric: took 7.089568888s to wait for apiserver health ...
I0415 19:05:08.149660    1546 cni.go:84] Creating CNI manager for ""
I0415 19:05:08.149676    1546 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0415 19:05:08.153164    1546 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0415 19:05:08.157054    1546 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0415 19:05:08.248462    1546 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0415 19:05:08.351593    1546 system_pods.go:43] waiting for kube-system pods to appear ...
I0415 19:05:08.441087    1546 system_pods.go:59] 8 kube-system pods found
I0415 19:05:08.441127    1546 system_pods.go:61] "coredns-5dd5756b68-2wlqh" [dc0090b5-b39e-43d8-a7ce-de18be7fc19a] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0415 19:05:08.441133    1546 system_pods.go:61] "etcd-minikube" [f9f78dc8-08f3-4ce0-93ae-d0f04dce4980] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0415 19:05:08.441140    1546 system_pods.go:61] "kube-apiserver-minikube" [a179077c-eee0-4252-9443-2415343fd7f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0415 19:05:08.441144    1546 system_pods.go:61] "kube-controller-manager-minikube" [030a0f9c-a9cc-4c64-908c-d521ebd5e456] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0415 19:05:08.441152    1546 system_pods.go:61] "kube-proxy-5t4wn" [af34d189-449d-477a-86ac-151923e832e6] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0415 19:05:08.441156    1546 system_pods.go:61] "kube-scheduler-minikube" [028e6940-60ab-40ae-bb42-5555d505d61a] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0415 19:05:08.441161    1546 system_pods.go:61] "metrics-server-7c66d45ddc-gt257" [d6918d58-6b4d-4391-89b6-6d17fc4b0d48] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0415 19:05:08.441163    1546 system_pods.go:61] "storage-provisioner" [fe4b1b8c-1282-4999-a2fa-fd8052206e7b] Running
I0415 19:05:08.441171    1546 system_pods.go:74] duration metric: took 89.563196ms to wait for pod list to return data ...
I0415 19:05:08.441179    1546 node_conditions.go:102] verifying NodePressure condition ...
I0415 19:05:08.446232    1546 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0415 19:05:08.446253    1546 node_conditions.go:123] node cpu capacity is 12
I0415 19:05:08.446277    1546 node_conditions.go:105] duration metric: took 5.091788ms to run NodePressure ...
I0415 19:05:08.446302    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0415 19:05:09.533783    1546 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.087452622s)
I0415 19:05:09.533815    1546 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0415 19:05:09.631795    1546 ops.go:34] apiserver oom_adj: -16
I0415 19:05:09.631818    1546 kubeadm.go:640] restartCluster took 22.840986574s
I0415 19:05:09.631832    1546 kubeadm.go:406] StartCluster complete in 22.865673992s
I0415 19:05:09.631862    1546 settings.go:142] acquiring lock: {Name:mkd04c5e12a1ddc2785ef6f15d2dbaac49342375 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0415 19:05:09.633691    1546 settings.go:150] Updating kubeconfig:  /home/aladin_257/.kube/config
I0415 19:05:09.635216    1546 lock.go:35] WriteFile acquiring /home/aladin_257/.kube/config: {Name:mk70a5a6896bb31776d2729d95acb49dad88a16e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0415 19:05:09.637426    1546 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0415 19:05:09.637594    1546 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0415 19:05:09.637772    1546 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0415 19:05:09.638005    1546 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0415 19:05:09.638037    1546 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0415 19:05:09.638047    1546 addons.go:69] Setting dashboard=true in profile "minikube"
I0415 19:05:09.638073    1546 addons.go:231] Setting addon dashboard=true in "minikube"
W0415 19:05:09.638084    1546 addons.go:240] addon dashboard should already be in state true
I0415 19:05:09.638204    1546 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0415 19:05:09.638249    1546 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0415 19:05:09.638286    1546 addons.go:240] addon storage-provisioner should already be in state true
I0415 19:05:09.638368    1546 host.go:66] Checking if "minikube" exists ...
I0415 19:05:09.638398    1546 host.go:66] Checking if "minikube" exists ...
I0415 19:05:09.638543    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:05:09.639217    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:05:09.639427    1546 addons.go:69] Setting metrics-server=true in profile "minikube"
I0415 19:05:09.639445    1546 addons.go:231] Setting addon metrics-server=true in "minikube"
W0415 19:05:09.639454    1546 addons.go:240] addon metrics-server should already be in state true
I0415 19:05:09.639503    1546 host.go:66] Checking if "minikube" exists ...
I0415 19:05:09.639968    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:05:09.640860    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:05:09.737840    1546 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0415 19:05:09.737903    1546 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0415 19:05:09.751002    1546 out.go:177] üîé  Verifying Kubernetes components...
I0415 19:05:09.760491    1546 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0415 19:05:09.883828    1546 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0415 19:05:09.883841    1546 addons.go:240] addon default-storageclass should already be in state true
I0415 19:05:09.883867    1546 host.go:66] Checking if "minikube" exists ...
I0415 19:05:09.884230    1546 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0415 19:05:09.891476    1546 out.go:177]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.4
I0415 19:05:09.894340    1546 addons.go:423] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0415 19:05:09.894370    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0415 19:05:09.896826    1546 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0415 19:05:09.894428    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:05:09.900863    1546 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0415 19:05:09.902920    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0415 19:05:09.902936    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0415 19:05:09.902996    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:05:09.943810    1546 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0415 19:05:09.947932    1546 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0415 19:05:09.947953    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0415 19:05:09.948064    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:05:10.017022    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:05:10.057904    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:05:10.059054    1546 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0415 19:05:10.059068    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0415 19:05:10.059153    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0415 19:05:10.101387    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:05:10.179508    1546 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51537 SSHKeyPath:/home/aladin_257/.minikube/machines/minikube/id_rsa Username:docker}
I0415 19:05:10.530103    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0415 19:05:10.530128    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0415 19:05:10.530162    1546 addons.go:423] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0415 19:05:10.530172    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0415 19:05:10.550903    1546 addons.go:423] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0415 19:05:10.550920    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0415 19:05:10.550899    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0415 19:05:10.550976    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0415 19:05:10.551119    1546 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0415 19:05:10.630383    1546 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0415 19:05:10.645670    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0415 19:05:10.645703    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0415 19:05:10.648579    1546 addons.go:423] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0415 19:05:10.648609    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0415 19:05:10.737355    1546 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0415 19:05:10.740603    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0415 19:05:10.740621    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0415 19:05:10.830194    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0415 19:05:10.830218    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0415 19:05:10.851193    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0415 19:05:10.851218    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0415 19:05:10.869009    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0415 19:05:10.869022    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0415 19:05:11.033868    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0415 19:05:11.033892    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0415 19:05:11.150058    1546 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0415 19:05:11.150083    1546 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0415 19:05:11.332892    1546 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0415 19:05:11.333612    1546 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.696147809s)
I0415 19:05:11.333711    1546 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0415 19:05:11.333825    1546 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.573295885s)
I0415 19:05:11.334326    1546 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0415 19:05:11.464962    1546 api_server.go:52] waiting for apiserver process to appear ...
I0415 19:05:11.465124    1546 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0415 19:05:16.373304    1546 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (5.040364943s)
I0415 19:05:16.376204    1546 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0415 19:05:16.373467    1546 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.743037028s)
I0415 19:05:16.373559    1546 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (5.636158312s)
I0415 19:05:16.373617    1546 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.822464991s)
I0415 19:05:16.373687    1546 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.908537544s)
I0415 19:05:16.378101    1546 api_server.go:72] duration metric: took 6.64013947s to wait for apiserver process to appear ...
I0415 19:05:16.378107    1546 api_server.go:88] waiting for apiserver healthz status ...
I0415 19:05:16.378119    1546 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51541/healthz ...
I0415 19:05:16.378134    1546 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0415 19:05:16.383324    1546 api_server.go:279] https://127.0.0.1:51541/healthz returned 200:
ok
I0415 19:05:16.386228    1546 out.go:177] üåü  Enabled addons: storage-provisioner, dashboard, metrics-server, default-storageclass
I0415 19:05:16.384640    1546 api_server.go:141] control plane version: v1.28.3
I0415 19:05:16.388005    1546 api_server.go:131] duration metric: took 9.888201ms to wait for apiserver health ...
I0415 19:05:16.388015    1546 system_pods.go:43] waiting for kube-system pods to appear ...
I0415 19:05:16.388018    1546 addons.go:502] enable addons completed in 6.75026808s: enabled=[storage-provisioner dashboard metrics-server default-storageclass]
I0415 19:05:16.394298    1546 system_pods.go:59] 8 kube-system pods found
I0415 19:05:16.394312    1546 system_pods.go:61] "coredns-5dd5756b68-2wlqh" [dc0090b5-b39e-43d8-a7ce-de18be7fc19a] Running
I0415 19:05:16.394314    1546 system_pods.go:61] "etcd-minikube" [f9f78dc8-08f3-4ce0-93ae-d0f04dce4980] Running
I0415 19:05:16.394322    1546 system_pods.go:61] "kube-apiserver-minikube" [a179077c-eee0-4252-9443-2415343fd7f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0415 19:05:16.394325    1546 system_pods.go:61] "kube-controller-manager-minikube" [030a0f9c-a9cc-4c64-908c-d521ebd5e456] Running
I0415 19:05:16.394328    1546 system_pods.go:61] "kube-proxy-5t4wn" [af34d189-449d-477a-86ac-151923e832e6] Running
I0415 19:05:16.394332    1546 system_pods.go:61] "kube-scheduler-minikube" [028e6940-60ab-40ae-bb42-5555d505d61a] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0415 19:05:16.394339    1546 system_pods.go:61] "metrics-server-7c66d45ddc-gt257" [d6918d58-6b4d-4391-89b6-6d17fc4b0d48] Running
I0415 19:05:16.394343    1546 system_pods.go:61] "storage-provisioner" [fe4b1b8c-1282-4999-a2fa-fd8052206e7b] Running
I0415 19:05:16.394347    1546 system_pods.go:74] duration metric: took 6.328141ms to wait for pod list to return data ...
I0415 19:05:16.394355    1546 kubeadm.go:581] duration metric: took 6.656397221s to wait for : map[apiserver:true system_pods:true] ...
I0415 19:05:16.394364    1546 node_conditions.go:102] verifying NodePressure condition ...
I0415 19:05:16.398210    1546 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0415 19:05:16.398226    1546 node_conditions.go:123] node cpu capacity is 12
I0415 19:05:16.398234    1546 node_conditions.go:105] duration metric: took 3.86704ms to run NodePressure ...
I0415 19:05:16.398243    1546 start.go:228] waiting for startup goroutines ...
I0415 19:05:16.398248    1546 start.go:233] waiting for cluster config update ...
I0415 19:05:16.398255    1546 start.go:242] writing updated cluster config ...
I0415 19:05:16.398544    1546 ssh_runner.go:195] Run: rm -f paused
I0415 19:05:16.559160    1546 start.go:600] kubectl: 1.28.4, cluster: 1.28.3 (minor skew: 0)
I0415 19:05:16.561300    1546 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Apr 16 07:35:27 minikube cri-dockerd[2124]: time="2024-04-16T07:35:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [===>                                               ]  2.314MB/32.04MB"
Apr 16 07:35:37 minikube cri-dockerd[2124]: time="2024-04-16T07:35:37Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [==========>                                        ]  10.44MB/51.26MB"
Apr 16 07:35:47 minikube cri-dockerd[2124]: time="2024-04-16T07:35:47Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [===========>                                       ]  7.606MB/32.04MB"
Apr 16 07:35:57 minikube cri-dockerd[2124]: time="2024-04-16T07:35:57Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [===============>                                   ]  10.25MB/32.04MB"
Apr 16 07:36:07 minikube cri-dockerd[2124]: time="2024-04-16T07:36:07Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [==================>                                ]  25.36MB/69.18MB"
Apr 16 07:36:17 minikube cri-dockerd[2124]: time="2024-04-16T07:36:17Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [===================>                               ]  26.44MB/69.18MB"
Apr 16 07:36:27 minikube cri-dockerd[2124]: time="2024-04-16T07:36:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [========================>                          ]  15.54MB/32.04MB"
Apr 16 07:36:37 minikube cri-dockerd[2124]: time="2024-04-16T07:36:37Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [=============================>                     ]  18.85MB/32.04MB"
Apr 16 07:36:47 minikube cri-dockerd[2124]: time="2024-04-16T07:36:47Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [========================>                          ]  33.45MB/69.18MB"
Apr 16 07:36:57 minikube cri-dockerd[2124]: time="2024-04-16T07:36:57Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [=====================================>             ]  23.81MB/32.04MB"
Apr 16 07:37:07 minikube cri-dockerd[2124]: time="2024-04-16T07:37:07Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [=========================>                         ]  35.61MB/69.18MB"
Apr 16 07:37:17 minikube cri-dockerd[2124]: time="2024-04-16T07:37:17Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [==========================>                        ]  36.69MB/69.18MB"
Apr 16 07:37:27 minikube cri-dockerd[2124]: time="2024-04-16T07:37:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [===========================================>       ]  28.11MB/32.04MB"
Apr 16 07:37:37 minikube cri-dockerd[2124]: time="2024-04-16T07:37:37Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Downloading [===============================================>   ]  30.76MB/32.04MB"
Apr 16 07:37:47 minikube cri-dockerd[2124]: time="2024-04-16T07:37:47Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [===========================>                       ]   28.2MB/51.26MB"
Apr 16 07:37:57 minikube cri-dockerd[2124]: time="2024-04-16T07:37:57Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [=============================>                     ]  29.77MB/51.26MB"
Apr 16 07:38:07 minikube cri-dockerd[2124]: time="2024-04-16T07:38:07Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [=================================>                 ]  46.41MB/69.18MB"
Apr 16 07:38:17 minikube cri-dockerd[2124]: time="2024-04-16T07:38:17Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [==================================>                ]  48.03MB/69.18MB"
Apr 16 07:38:27 minikube cri-dockerd[2124]: time="2024-04-16T07:38:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [===================================>               ]   49.1MB/69.18MB"
Apr 16 07:38:37 minikube cri-dockerd[2124]: time="2024-04-16T07:38:37Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [====================================>              ]  50.72MB/69.18MB"
Apr 16 07:38:47 minikube cri-dockerd[2124]: time="2024-04-16T07:38:47Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [=====================================>             ]  52.34MB/69.18MB"
Apr 16 07:38:57 minikube cri-dockerd[2124]: time="2024-04-16T07:38:57Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [======================================>            ]  53.96MB/69.18MB"
Apr 16 07:39:07 minikube cri-dockerd[2124]: time="2024-04-16T07:39:07Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [=====================================>             ]  38.64MB/51.26MB"
Apr 16 07:39:17 minikube cri-dockerd[2124]: time="2024-04-16T07:39:17Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [=======================================>           ]  40.21MB/51.26MB"
Apr 16 07:39:27 minikube cri-dockerd[2124]: time="2024-04-16T07:39:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [==========================================>        ]  58.82MB/69.18MB"
Apr 16 07:39:37 minikube cri-dockerd[2124]: time="2024-04-16T07:39:37Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [============================================>      ]  62.06MB/69.18MB"
Apr 16 07:39:47 minikube cri-dockerd[2124]: time="2024-04-16T07:39:47Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [==============================================>    ]  64.21MB/69.18MB"
Apr 16 07:39:57 minikube cri-dockerd[2124]: time="2024-04-16T07:39:57Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Downloading [================================================>  ]  66.91MB/69.18MB"
Apr 16 07:40:07 minikube cri-dockerd[2124]: time="2024-04-16T07:40:07Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 8a0361ad29a5: Downloading [================================================>  ]  50.13MB/51.26MB"
Apr 16 07:40:17 minikube cri-dockerd[2124]: time="2024-04-16T07:40:17Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82c7c8f7c1ec: Extracting [==============================>                    ]  42.34MB/69.18MB"
Apr 16 07:40:27 minikube cri-dockerd[2124]: time="2024-04-16T07:40:27Z" level=info msg="Pulling image registry.k8s.io/hpa-example:latest: 82174a5610d6: Extracting [=============================================>     ]  29.16MB/32.04MB"
Apr 16 07:40:37 minikube cri-dockerd[2124]: time="2024-04-16T07:40:37Z" level=info msg="Stop pulling image registry.k8s.io/hpa-example:latest: Status: Downloaded newer image for registry.k8s.io/hpa-example:latest"
Apr 17 11:13:05 minikube dockerd[1798]: time="2024-04-17T11:13:05.250528855Z" level=info msg="ignoring event" container=5cd133fa4696a4dfdfe2ce96d216d9e5b9f2f82a12c76725100af2f3ad2b79b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:05 minikube dockerd[1798]: time="2024-04-17T11:13:05.255538174Z" level=info msg="ignoring event" container=68b3b604f5e203165e29b8a3b235e3b47859ee22fc353f8e8e421027a2f1d3e4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:05 minikube dockerd[1798]: time="2024-04-17T11:13:05.554508536Z" level=info msg="ignoring event" container=49bbd6b588f77fe1377f52b83923f9f312278f669cb29c83f876ad39700506b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:06 minikube dockerd[1798]: time="2024-04-17T11:13:06.343349482Z" level=info msg="ignoring event" container=9c0784272a5978cf5e4ed103e6126643003f0bd3c7f485757e004861a9f8ee14 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:06 minikube dockerd[1798]: time="2024-04-17T11:13:06.420617678Z" level=info msg="ignoring event" container=506bbacded7d0436be9d210c130fa427132f2da7d57e71b1fff868d60f94e8f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:06 minikube dockerd[1798]: time="2024-04-17T11:13:06.427271180Z" level=info msg="ignoring event" container=8d788c1e3b7cf9d6637c56bc97099084f92c56ad9722332ee848d60e3e3507f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:18 minikube dockerd[1798]: time="2024-04-17T11:13:18.693609912Z" level=info msg="ignoring event" container=497f9179dd7aa18947a8b817c7499257333174fb432d98e5e1b7b3d2b3e11420 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:19 minikube cri-dockerd[2124]: time="2024-04-17T11:13:19Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"php-apache-598b474864-lnb5p_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Apr 17 11:13:19 minikube dockerd[1798]: time="2024-04-17T11:13:19.081761976Z" level=info msg="ignoring event" container=71084c7df86baeaf55af6bf92221475ca82d0c0e7661502525637c2a14464dc4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.249251791Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=ea603f3cd407c69d7b9ab57584e36fdf512ccd6ebd8674b731592e64f2be7efe
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.249256250Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=da9bbf36385781895c1edd7515ee8e9c93e78de543ecd090c0067f8ec443afe1
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.369966502Z" level=info msg="ignoring event" container=ea603f3cd407c69d7b9ab57584e36fdf512ccd6ebd8674b731592e64f2be7efe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.430300172Z" level=info msg="ignoring event" container=da9bbf36385781895c1edd7515ee8e9c93e78de543ecd090c0067f8ec443afe1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:38 minikube cri-dockerd[2124]: time="2024-04-17T11:13:38Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"redis-follower-7dddf7c979-msggj_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.885815298Z" level=info msg="ignoring event" container=0c5470009ecf3fd4a48a2555abc460eefc277be7cd36e9c02d1536ff2dca94a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:38 minikube dockerd[1798]: time="2024-04-17T11:13:38.977355172Z" level=info msg="ignoring event" container=dc114d3a2791eccea79ff1592ad1b092a82ff68a4116dc259002d185bb4bac7c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:13:57 minikube cri-dockerd[2124]: time="2024-04-17T11:13:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8de922a0a57fa86acd22d9de839a3dd7811a606bb18fc208e5065bbdc0012937/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 17 11:13:57 minikube cri-dockerd[2124]: time="2024-04-17T11:13:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9685d88dd7ebe027650ecdf8301ee7ead5d12dfb235978b3524c30d28b5ac540/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 17 11:13:57 minikube cri-dockerd[2124]: time="2024-04-17T11:13:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/345f2e4420bf4b41fd48d48740f557304d17c093ef7dff136f54566d3898fa60/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.217288799Z" level=info msg="ignoring event" container=365e88941c89b0518c92fe237ca4084379c42d22c4cc290bc19f0fb9e889b3b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.217991474Z" level=info msg="ignoring event" container=842bcad8c52afad97c6c6493e15665f69af058e176778d2be5af6fe529d4841f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.218403862Z" level=info msg="ignoring event" container=f8100f85bc8467520bacfba5ad16add7679ba1792a7cb09a384b470705c0e058 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.616150750Z" level=info msg="ignoring event" container=345f2e4420bf4b41fd48d48740f557304d17c093ef7dff136f54566d3898fa60 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.657056378Z" level=info msg="ignoring event" container=9685d88dd7ebe027650ecdf8301ee7ead5d12dfb235978b3524c30d28b5ac540 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:15 minikube dockerd[1798]: time="2024-04-17T11:15:15.750813805Z" level=info msg="ignoring event" container=8de922a0a57fa86acd22d9de839a3dd7811a606bb18fc208e5065bbdc0012937 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 17 11:15:58 minikube cri-dockerd[2124]: time="2024-04-17T11:15:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6877a718b44a18c1325a78385d7d64d2f788f7bf5752ac0fbd60742014b61d3b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 17 11:15:58 minikube cri-dockerd[2124]: time="2024-04-17T11:15:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c72dc04d2900615d388db6431b6a9db719de97bd941721ee2d0ad1e170673824/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 17 11:15:58 minikube cri-dockerd[2124]: time="2024-04-17T11:15:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e0ba803060622b4efb21534b82fc1149647d7bfb2830b93c4812302b6cf9119/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
da834ea40f582       c62cc8d0cd782       6 hours ago         Running             php-redis                   0                   4e0ba80306062       frontend-795b566649-cxnbl
01a78f9a5d714       c62cc8d0cd782       6 hours ago         Running             php-redis                   0                   c72dc04d29006       frontend-795b566649-ndxnf
3d836c836625e       c62cc8d0cd782       6 hours ago         Running             php-redis                   0                   6877a718b44a1       frontend-795b566649-447g5
2b390df4e33a7       07655ddf2eebe       2 days ago          Running             kubernetes-dashboard        28                  ec08d072514e7       kubernetes-dashboard-8694d4445c-h6whs
e75421dbc807f       6e38f40d628db       2 days ago          Running             storage-provisioner         39                  3bb3d29bd6f91       storage-provisioner
55d0fccc0a169       a608c686bac93       2 days ago          Running             metrics-server              9                   ef2ac3e7d1964       metrics-server-7c66d45ddc-gt257
f89427938a72d       115053965e86b       2 days ago          Running             dashboard-metrics-scraper   19                  ce690a0fe0115       dashboard-metrics-scraper-7fd5cb4ddc-hs5ld
1a4741fbfc142       ead0a4a53df89       2 days ago          Running             coredns                     23                  502508041b776       coredns-5dd5756b68-2wlqh
7ac64c8d40519       07655ddf2eebe       2 days ago          Exited              kubernetes-dashboard        27                  ec08d072514e7       kubernetes-dashboard-8694d4445c-h6whs
c23261166b18c       bfc896cf80fba       2 days ago          Running             kube-proxy                  24                  d43897bdf38d1       kube-proxy-5t4wn
66d3adf39c8ce       6e38f40d628db       2 days ago          Exited              storage-provisioner         38                  3bb3d29bd6f91       storage-provisioner
010e3a144e408       6d1b4fd1b182d       2 days ago          Running             kube-scheduler              24                  65b7e1675341b       kube-scheduler-minikube
736797f5de2db       10baa1ca17068       2 days ago          Running             kube-controller-manager     25                  c5128b3852b31       kube-controller-manager-minikube
57977577d42e2       73deb9a3f7025       2 days ago          Running             etcd                        24                  417d22af1048e       etcd-minikube
51cdadf5ade12       5374347291230       2 days ago          Running             kube-apiserver              23                  b4efb76ba824d       kube-apiserver-minikube
867b1d82bf32a       115053965e86b       2 days ago          Exited              dashboard-metrics-scraper   18                  052a9470e66ed       dashboard-metrics-scraper-7fd5cb4ddc-hs5ld
de8e38428a2f4       ead0a4a53df89       2 days ago          Exited              coredns                     22                  45c45dcf326c6       coredns-5dd5756b68-2wlqh
9f2039bd62730       a608c686bac93       2 days ago          Exited              metrics-server              8                   2ad4cbaa59181       metrics-server-7c66d45ddc-gt257
669400216a1e2       bfc896cf80fba       2 days ago          Exited              kube-proxy                  23                  41f98e79bd94e       kube-proxy-5t4wn
f11c9c7d3e23c       6d1b4fd1b182d       2 days ago          Exited              kube-scheduler              23                  b6dbd30804d98       kube-scheduler-minikube
a33ec903e7089       10baa1ca17068       2 days ago          Exited              kube-controller-manager     24                  67af5c8128573       kube-controller-manager-minikube
4c80463254fd8       73deb9a3f7025       2 days ago          Exited              etcd                        23                  5daff855ef22f       etcd-minikube
a3d14ef9360b8       5374347291230       2 days ago          Exited              kube-apiserver              22                  4f5edf02838ed       kube-apiserver-minikube

* 
* ==> coredns [1a4741fbfc14] <==
* [INFO] 10.244.0.169:42726 - 43536 "AAAA IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000249055s
[INFO] 10.244.0.169:42726 - 39942 "A IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000351243s
[INFO] 10.244.0.169:43800 - 4804 "AAAA IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000163882s
[INFO] 10.244.0.169:43800 - 32953 "A IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000244997s
[INFO] 10.244.0.170:41849 - 8505 "A IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000182357s
[INFO] 10.244.0.170:41849 - 53835 "AAAA IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000160727s
[INFO] 10.244.0.170:54090 - 22182 "A IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000133704s
[INFO] 10.244.0.170:54090 - 29874 "AAAA IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.0001679s
[INFO] 10.244.0.170:36405 - 46219 "A IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000167409s
[INFO] 10.244.0.170:36405 - 11671 "AAAA IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000126611s
[INFO] 10.244.0.169:53060 - 4798 "AAAA IN redis-leader. udp 30 false 512" - - 0 2.001405201s
[ERROR] plugin/errors: 2 redis-leader. AAAA: read udp 10.244.0.162:39259->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:53060 - 39094 "A IN redis-leader. udp 30 false 512" - - 0 2.001336124s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:59980->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.170:48422 - 7951 "A IN redis-leader. udp 30 false 512" - - 0 2.001398983s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:40991->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:53060 - 39094 "A IN redis-leader. udp 30 false 512" - - 0 2.001511178s
[INFO] 10.244.0.169:53060 - 4798 "AAAA IN redis-leader. udp 30 false 512" - - 0 2.001615577s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:45111->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-leader. AAAA: read udp 10.244.0.162:44071->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:55226 - 47208 "A IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000246419s
[INFO] 10.244.0.169:55226 - 18051 "AAAA IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.00042032s
[INFO] 10.244.0.169:34596 - 8533 "AAAA IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.00009997s
[INFO] 10.244.0.169:34596 - 28234 "A IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000109168s
[INFO] 10.244.0.169:55451 - 10488 "AAAA IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000163812s
[INFO] 10.244.0.169:55451 - 13293 "A IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000288144s
[INFO] 10.244.0.169:54064 - 28527 "AAAA IN redis-leader. udp 30 false 512" - - 0 2.001368771s
[INFO] 10.244.0.169:54064 - 50025 "A IN redis-leader. udp 30 false 512" - - 0 2.00137557s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:48532->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-leader. AAAA: read udp 10.244.0.162:34074->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.170:48422 - 7951 "A IN redis-leader. udp 30 false 512" - - 0 2.000525716s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:54096->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:54064 - 50025 "A IN redis-leader. udp 30 false 512" - - 0 2.032354418s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:34254->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:54064 - 28527 "AAAA IN redis-leader. udp 30 false 512" - - 0 2.032386074s
[ERROR] plugin/errors: 2 redis-leader. AAAA: read udp 10.244.0.162:40703->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:49447 - 29723 "AAAA IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000205672s
[INFO] 10.244.0.169:49447 - 51725 "A IN redis-leader.default.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000315655s
[INFO] 10.244.0.169:53802 - 10184 "AAAA IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000136189s
[INFO] 10.244.0.169:53802 - 51131 "A IN redis-leader.svc.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000259423s
[INFO] 10.244.0.169:34782 - 46522 "AAAA IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.00013166s
[INFO] 10.244.0.169:34782 - 9136 "A IN redis-leader.cluster.local. udp 44 false 512" NXDOMAIN qr,aa,rd 137 0.000234245s
[INFO] 10.244.0.169:44076 - 9385 "A IN redis-leader. udp 30 false 512" - - 0 2.000651165s
[ERROR] plugin/errors: 2 redis-leader. A: read udp 10.244.0.162:60730->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.169:44076 - 53422 "AAAA IN redis-leader. udp 30 false 512" - - 0 2.00073564s
[ERROR] plugin/errors: 2 redis-leader. AAAA: read udp 10.244.0.162:54144->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.183:37171 - 62134 "A IN redis-follower.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.001424512s
[INFO] 10.244.0.183:37171 - 7633 "AAAA IN redis-follower.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.001755716s
[INFO] 10.244.0.183:60088 - 61184 "AAAA IN redis-follower.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000363591s
[INFO] 10.244.0.183:60088 - 13557 "A IN redis-follower.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000571732s
[INFO] 10.244.0.183:39520 - 39779 "AAAA IN redis-follower.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000188899s
[INFO] 10.244.0.183:39520 - 42584 "A IN redis-follower.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000309478s
[INFO] 10.244.0.183:46819 - 4501 "AAAA IN redis-follower. udp 32 false 512" - - 0 2.00686767s
[INFO] 10.244.0.183:46819 - 47245 "A IN redis-follower. udp 32 false 512" - - 0 2.006869373s
[ERROR] plugin/errors: 2 redis-follower. AAAA: read udp 10.244.0.162:48120->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-follower. A: read udp 10.244.0.162:52671->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.183:46819 - 47245 "A IN redis-follower. udp 32 false 512" - - 0 2.000537807s
[ERROR] plugin/errors: 2 redis-follower. A: read udp 10.244.0.162:42499->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.183:46819 - 4501 "AAAA IN redis-follower. udp 32 false 512" - - 0 2.000611181s
[ERROR] plugin/errors: 2 redis-follower. AAAA: read udp 10.244.0.162:43704->192.168.65.254:53: i/o timeout

* 
* ==> coredns [de8e38428a2f] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:47294 - 19182 "HINFO IN 8865307943722458486.745823630406062156. udp 56 false 512" NXDOMAIN qr,rd,ra 56 3.41447895s
[INFO] 127.0.0.1:57819 - 9039 "HINFO IN 8865307943722458486.745823630406062156. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.499766693s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_01T18_38_58_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 01 Mar 2024 17:35:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 17 Apr 2024 17:40:02 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 17 Apr 2024 17:37:03 +0000   Fri, 01 Mar 2024 17:35:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 17 Apr 2024 17:37:03 +0000   Fri, 01 Mar 2024 17:35:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 17 Apr 2024 17:37:03 +0000   Fri, 01 Mar 2024 17:35:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 17 Apr 2024 17:37:03 +0000   Fri, 01 Mar 2024 17:39:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7027864Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7027864Ki
  pods:               110
System Info:
  Machine ID:                 8e76b253860e44bb958316792f458d6a
  System UUID:                8e76b253860e44bb958316792f458d6a
  Boot ID:                    af721f9b-9c23-4610-9347-ea3e4ed5b351
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     frontend-795b566649-447g5                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         6h24m
  default                     frontend-795b566649-cxnbl                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         6h24m
  default                     frontend-795b566649-ndxnf                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         6h24m
  kube-system                 coredns-5dd5756b68-2wlqh                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     47d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         47d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47d
  kube-system                 kube-proxy-5t4wn                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47d
  kube-system                 metrics-server-7c66d45ddc-gt257               100m (0%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         42d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         47d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-hs5ld    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         45d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-h6whs         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         45d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1150m (9%!)(MISSING)  0 (0%!)(MISSING)
  memory             670Mi (9%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000002] RDX: 000055a4fe1775e0 RSI: 000055a4fe127290 RDI: 0000000000000000
[  +0.000002] RBP: 0000000000000008 R08: 0000000000000000 R09: 0000000000000000
[  +0.000023] R10: 0000000000000000 R11: 00007f061fc013d0 R12: 0000000000000000
[  +0.000002] R13: 0000000000000000 R14: 0000000000000018 R15: 0000000000000001
[  +0.000001] FS:  00007f061c786ec0 GS:  0000000000000000
[  +1.255315] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003291] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001047] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000993] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001310] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000567] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000688] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002960] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +5.146861] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.359135] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 13:56] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.140712] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 14:33] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.855298] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 14:35] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.248032] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 15:19] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.375398] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 15:48] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.172197] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 15:55] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001212] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001099] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001462] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002437] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001005] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001144] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001480] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +3.513335] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000979] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001043] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001189] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002030] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001240] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001173] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001251] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr17 16:41] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002657] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000757] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001544] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002215] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000961] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000772] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001001] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr17 16:46] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000963] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001145] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001231] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001898] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001541] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001281] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000893] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr17 16:47] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.295580] WSL (1) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[Apr17 17:34] hrtimer: interrupt took 490613 ns

* 
* ==> etcd [4c80463254fd] <==
* {"level":"warn","ts":"2024-04-15T16:28:46.935004Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.627951Z","time spent":"306.990012ms","remote":"127.0.0.1:47510","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1398,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/kubernetes-dashboard/kubernetes-dashboard-xbngn\" mod_revision:343809 > success:<request_put:<key:\"/registry/endpointslices/kubernetes-dashboard/kubernetes-dashboard-xbngn\" value_size:1318 >> failure:<request_range:<key:\"/registry/endpointslices/kubernetes-dashboard/kubernetes-dashboard-xbngn\" > >"}
{"level":"warn","ts":"2024-04-15T16:28:46.935367Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.629214Z","time spent":"306.106297ms","remote":"127.0.0.1:47644","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4106,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/deployments/kube-system/coredns\" mod_revision:390161 > success:<request_put:<key:\"/registry/deployments/kube-system/coredns\" value_size:4057 >> failure:<request_range:<key:\"/registry/deployments/kube-system/coredns\" > >"}
{"level":"warn","ts":"2024-04-15T16:28:46.934756Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.627074Z","time spent":"307.613724ms","remote":"127.0.0.1:47510","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1507,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/kube-system/metrics-server-qv4h8\" mod_revision:343822 > success:<request_put:<key:\"/registry/endpointslices/kube-system/metrics-server-qv4h8\" value_size:1442 >> failure:<request_range:<key:\"/registry/endpointslices/kube-system/metrics-server-qv4h8\" > >"}
{"level":"warn","ts":"2024-04-15T16:28:46.934996Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.628121Z","time spent":"306.666894ms","remote":"127.0.0.1:47510","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1433,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/kubernetes-dashboard/dashboard-metrics-scraper-zdxwl\" mod_revision:343810 > success:<request_put:<key:\"/registry/endpointslices/kubernetes-dashboard/dashboard-metrics-scraper-zdxwl\" value_size:1348 >> failure:<request_range:<key:\"/registry/endpointslices/kubernetes-dashboard/dashboard-metrics-scraper-zdxwl\" > >"}
{"level":"info","ts":"2024-04-15T16:28:46.937248Z","caller":"traceutil/trace.go:171","msg":"trace[2003001973] transaction","detail":"{read_only:false; response_revision:390213; number_of_response:1; }","duration":"306.918279ms","start":"2024-04-15T16:28:46.630297Z","end":"2024-04-15T16:28:46.937215Z","steps":["trace[2003001973] 'process raft request'  (duration: 306.498591ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:28:46.93745Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.630277Z","time spent":"307.026235ms","remote":"127.0.0.1:47510","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1292,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/endpointslices/kube-system/kube-dns-2np4z\" mod_revision:343811 > success:<request_put:<key:\"/registry/endpointslices/kube-system/kube-dns-2np4z\" value_size:1233 >> failure:<request_range:<key:\"/registry/endpointslices/kube-system/kube-dns-2np4z\" > >"}
{"level":"warn","ts":"2024-04-15T16:28:46.937838Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"311.193851ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kube-system/kube-dns\" ","response":"range_response_count:1 size:1212"}
{"level":"warn","ts":"2024-04-15T16:28:46.937854Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.908198ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/coredns-5dd5756b68-2wlqh.17c680fac11de752\" ","response":"range_response_count:1 size:838"}
{"level":"info","ts":"2024-04-15T16:28:46.937888Z","caller":"traceutil/trace.go:171","msg":"trace[247462928] range","detail":"{range_begin:/registry/services/specs/kube-system/kube-dns; range_end:; response_count:1; response_revision:390213; }","duration":"311.245565ms","start":"2024-04-15T16:28:46.626628Z","end":"2024-04-15T16:28:46.937873Z","steps":["trace[247462928] 'agreement among raft nodes before linearized reading'  (duration: 311.151797ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:46.937919Z","caller":"traceutil/trace.go:171","msg":"trace[292895913] range","detail":"{range_begin:/registry/events/kube-system/coredns-5dd5756b68-2wlqh.17c680fac11de752; range_end:; response_count:1; response_revision:390213; }","duration":"204.992858ms","start":"2024-04-15T16:28:46.732908Z","end":"2024-04-15T16:28:46.937901Z","steps":["trace[292895913] 'agreement among raft nodes before linearized reading'  (duration: 204.863969ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:28:46.937932Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.626269Z","time spent":"311.652913ms","remote":"127.0.0.1:47420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":47,"response count":1,"response size":1236,"request content":"key:\"/registry/services/specs/kube-system/kube-dns\" "}
{"level":"warn","ts":"2024-04-15T16:28:46.938437Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"313.754715ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/kube-system/kube-proxy\" ","response":"range_response_count:1 size:2897"}
{"level":"info","ts":"2024-04-15T16:28:46.938487Z","caller":"traceutil/trace.go:171","msg":"trace[1646051862] range","detail":"{range_begin:/registry/daemonsets/kube-system/kube-proxy; range_end:; response_count:1; response_revision:390213; }","duration":"313.811569ms","start":"2024-04-15T16:28:46.624663Z","end":"2024-04-15T16:28:46.938474Z","steps":["trace[1646051862] 'agreement among raft nodes before linearized reading'  (duration: 313.687139ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:28:46.938526Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.624636Z","time spent":"313.879836ms","remote":"127.0.0.1:47654","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":2921,"request content":"key:\"/registry/daemonsets/kube-system/kube-proxy\" "}
{"level":"warn","ts":"2024-04-15T16:28:46.939307Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"209.922926ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-csrf\" ","response":"range_response_count:1 size:1045"}
{"level":"info","ts":"2024-04-15T16:28:46.939369Z","caller":"traceutil/trace.go:171","msg":"trace[1466594795] range","detail":"{range_begin:/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-csrf; range_end:; response_count:1; response_revision:390213; }","duration":"209.989851ms","start":"2024-04-15T16:28:46.729359Z","end":"2024-04-15T16:28:46.939349Z","steps":["trace[1466594795] 'agreement among raft nodes before linearized reading'  (duration: 208.649027ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:46.940952Z","caller":"traceutil/trace.go:171","msg":"trace[1130792597] linearizableReadLoop","detail":"{readStateIndex:489064; appliedIndex:489059; }","duration":"203.819636ms","start":"2024-04-15T16:28:46.733516Z","end":"2024-04-15T16:28:46.937336Z","steps":["trace[1130792597] 'read index received'  (duration: 52.806¬µs)","trace[1130792597] 'applied index is now lower than readState.Index'  (duration: 203.763834ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-15T16:28:47.024345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.96406ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:1436"}
{"level":"warn","ts":"2024-04-15T16:28:47.025729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"399.38839ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard\" ","response":"range_response_count:1 size:1396"}
{"level":"warn","ts":"2024-04-15T16:28:47.028476Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"402.07382ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/specs/kube-system/metrics-server\" ","response":"range_response_count:1 size:1657"}
{"level":"info","ts":"2024-04-15T16:28:47.028566Z","caller":"traceutil/trace.go:171","msg":"trace[644271818] range","detail":"{range_begin:/registry/services/specs/kube-system/metrics-server; range_end:; response_count:1; response_revision:390213; }","duration":"402.171696ms","start":"2024-04-15T16:28:46.62637Z","end":"2024-04-15T16:28:47.028542Z","steps":["trace[644271818] 'agreement among raft nodes before linearized reading'  (duration: 311.117848ms)","trace[644271818] 'range keys from in-memory index tree'  (duration: 90.898847ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-15T16:28:47.028627Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.626352Z","time spent":"402.260635ms","remote":"127.0.0.1:47420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":53,"response count":1,"response size":1681,"request content":"key:\"/registry/services/specs/kube-system/metrics-server\" "}
{"level":"info","ts":"2024-04-15T16:28:47.0289Z","caller":"traceutil/trace.go:171","msg":"trace[741447583] transaction","detail":"{read_only:false; response_revision:390214; number_of_response:1; }","duration":"102.567023ms","start":"2024-04-15T16:28:46.926306Z","end":"2024-04-15T16:28:47.028873Z","steps":["trace[741447583] 'process raft request'  (duration: 10.606304ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:47.0351Z","caller":"traceutil/trace.go:171","msg":"trace[518650166] transaction","detail":"{read_only:false; response_revision:390215; number_of_response:1; }","duration":"108.427811ms","start":"2024-04-15T16:28:46.926629Z","end":"2024-04-15T16:28:47.035057Z","steps":["trace[518650166] 'process raft request'  (duration: 102.072456ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:28:47.035152Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.796124ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io\" ","response":"range_response_count:1 size:2524"}
{"level":"info","ts":"2024-04-15T16:28:47.035298Z","caller":"traceutil/trace.go:171","msg":"trace[1492086268] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/v1beta1.metrics.k8s.io; range_end:; response_count:1; response_revision:390215; }","duration":"200.88416ms","start":"2024-04-15T16:28:46.834322Z","end":"2024-04-15T16:28:47.035207Z","steps":["trace[1492086268] 'agreement among raft nodes before linearized reading'  (duration: 200.657415ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:47.123946Z","caller":"traceutil/trace.go:171","msg":"trace[2116530038] range","detail":"{range_begin:/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:390213; }","duration":"398.344304ms","start":"2024-04-15T16:28:46.626174Z","end":"2024-04-15T16:28:47.024518Z","steps":["trace[2116530038] 'agreement among raft nodes before linearized reading'  (duration: 311.468097ms)","trace[2116530038] 'range keys from bolt db'  (duration: 84.735427ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-15T16:28:47.124127Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.626157Z","time spent":"497.916893ms","remote":"127.0.0.1:47420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":73,"response count":1,"response size":1460,"request content":"key:\"/registry/services/specs/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"info","ts":"2024-04-15T16:28:47.124764Z","caller":"traceutil/trace.go:171","msg":"trace[209786045] range","detail":"{range_begin:/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard; range_end:; response_count:1; response_revision:390213; }","duration":"399.516308ms","start":"2024-04-15T16:28:46.626265Z","end":"2024-04-15T16:28:47.025781Z","steps":["trace[209786045] 'agreement among raft nodes before linearized reading'  (duration: 311.366142ms)","trace[209786045] 'range keys from bolt db'  (duration: 87.846917ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-15T16:28:47.124846Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:28:46.626238Z","time spent":"498.585716ms","remote":"127.0.0.1:47420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":1,"response size":1420,"request content":"key:\"/registry/services/specs/kubernetes-dashboard/kubernetes-dashboard\" "}
{"level":"warn","ts":"2024-04-15T16:28:47.325475Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.569997ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028531336212066 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/deployments/kube-system/metrics-server\" mod_revision:343826 > success:<request_put:<key:\"/registry/deployments/kube-system/metrics-server\" value_size:5295 >> failure:<request_range:<key:\"/registry/deployments/kube-system/metrics-server\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-15T16:28:47.325718Z","caller":"traceutil/trace.go:171","msg":"trace[614929947] transaction","detail":"{read_only:false; response_revision:390218; number_of_response:1; }","duration":"191.194171ms","start":"2024-04-15T16:28:47.134501Z","end":"2024-04-15T16:28:47.325695Z","steps":["trace[614929947] 'process raft request'  (duration: 90.290163ms)","trace[614929947] 'compare'  (duration: 100.434044ms)"],"step_count":2}
{"level":"info","ts":"2024-04-15T16:28:47.326082Z","caller":"traceutil/trace.go:171","msg":"trace[1257509396] transaction","detail":"{read_only:false; response_revision:390219; number_of_response:1; }","duration":"191.010713ms","start":"2024-04-15T16:28:47.135019Z","end":"2024-04-15T16:28:47.32603Z","steps":["trace[1257509396] 'process raft request'  (duration: 190.572758ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:47.332553Z","caller":"traceutil/trace.go:171","msg":"trace[1306381882] transaction","detail":"{read_only:false; response_revision:390220; number_of_response:1; }","duration":"197.194839ms","start":"2024-04-15T16:28:47.135328Z","end":"2024-04-15T16:28:47.332523Z","steps":["trace[1306381882] 'process raft request'  (duration: 190.563544ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:49.527146Z","caller":"traceutil/trace.go:171","msg":"trace[1878195662] transaction","detail":"{read_only:false; response_revision:390221; number_of_response:1; }","duration":"100.387549ms","start":"2024-04-15T16:28:49.426726Z","end":"2024-04-15T16:28:49.527114Z","steps":["trace[1878195662] 'process raft request'  (duration: 100.176264ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:49.937344Z","caller":"traceutil/trace.go:171","msg":"trace[79285375] transaction","detail":"{read_only:false; response_revision:390226; number_of_response:1; }","duration":"108.716611ms","start":"2024-04-15T16:28:49.828608Z","end":"2024-04-15T16:28:49.937325Z","steps":["trace[79285375] 'process raft request'  (duration: 108.38035ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:28:49.937572Z","caller":"traceutil/trace.go:171","msg":"trace[1423480631] transaction","detail":"{read_only:false; response_revision:390225; number_of_response:1; }","duration":"108.766215ms","start":"2024-04-15T16:28:49.828287Z","end":"2024-04-15T16:28:49.937053Z","steps":["trace[1423480631] 'process raft request'  (duration: 16.754052ms)","trace[1423480631] 'compare'  (duration: 91.700888ms)"],"step_count":2}
{"level":"info","ts":"2024-04-15T16:33:22.44381Z","caller":"traceutil/trace.go:171","msg":"trace[969776876] transaction","detail":"{read_only:false; response_revision:390443; number_of_response:1; }","duration":"116.804221ms","start":"2024-04-15T16:33:22.326972Z","end":"2024-04-15T16:33:22.443777Z","steps":["trace[969776876] 'process raft request'  (duration: 116.629897ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:37:54.435168Z","caller":"traceutil/trace.go:171","msg":"trace[1475183488] transaction","detail":"{read_only:false; response_revision:390658; number_of_response:1; }","duration":"111.740808ms","start":"2024-04-15T16:37:54.323396Z","end":"2024-04-15T16:37:54.435137Z","steps":["trace[1475183488] 'process raft request'  (duration: 111.557958ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:38:15.419378Z","caller":"traceutil/trace.go:171","msg":"trace[771902742] transaction","detail":"{read_only:false; response_revision:390674; number_of_response:1; }","duration":"186.2722ms","start":"2024-04-15T16:38:15.23307Z","end":"2024-04-15T16:38:15.419342Z","steps":["trace[771902742] 'process raft request'  (duration: 186.019947ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:38:20.537264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":390441}
{"level":"info","ts":"2024-04-15T16:38:20.583969Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":390441,"took":"46.27495ms","hash":2365383233}
{"level":"info","ts":"2024-04-15T16:38:20.584127Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2365383233,"revision":390441,"compact-revision":389785}
{"level":"info","ts":"2024-04-15T16:38:36.321607Z","caller":"traceutil/trace.go:171","msg":"trace[1071611023] transaction","detail":"{read_only:false; response_revision:390692; number_of_response:1; }","duration":"187.816652ms","start":"2024-04-15T16:38:36.133515Z","end":"2024-04-15T16:38:36.321331Z","steps":["trace[1071611023] 'process raft request'  (duration: 187.538913ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:39:07.634061Z","caller":"traceutil/trace.go:171","msg":"trace[1943830869] transaction","detail":"{read_only:false; response_revision:390718; number_of_response:1; }","duration":"109.967015ms","start":"2024-04-15T16:39:07.524042Z","end":"2024-04-15T16:39:07.634009Z","steps":["trace[1943830869] 'process raft request'  (duration: 109.620095ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:43:20.542876Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":390680}
{"level":"info","ts":"2024-04-15T16:43:20.544192Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":390680,"took":"1.046623ms","hash":3143244937}
{"level":"info","ts":"2024-04-15T16:43:20.544244Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3143244937,"revision":390680,"compact-revision":390441}
{"level":"info","ts":"2024-04-15T16:45:00.848182Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":490049,"local-member-snapshot-index":480048,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-04-15T16:45:00.859653Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":490049}
{"level":"info","ts":"2024-04-15T16:45:00.859943Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":485049}
{"level":"warn","ts":"2024-04-15T16:45:15.014093Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.231487ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2024-04-15T16:45:15.014191Z","caller":"traceutil/trace.go:171","msg":"trace[465803705] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:391010; }","duration":"120.394848ms","start":"2024-04-15T16:45:14.893782Z","end":"2024-04-15T16:45:15.014177Z","steps":["trace[465803705] 'range keys from in-memory index tree'  (duration: 120.137728ms)"],"step_count":1}
{"level":"info","ts":"2024-04-15T16:45:25.188663Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000018-000000000006b6ec.snap"}
{"level":"info","ts":"2024-04-15T16:45:31.261849Z","caller":"traceutil/trace.go:171","msg":"trace[2043981207] linearizableReadLoop","detail":"{readStateIndex:490074; appliedIndex:490073; }","duration":"368.249634ms","start":"2024-04-15T16:45:30.893529Z","end":"2024-04-15T16:45:31.261779Z","steps":["trace[2043981207] 'read index received'  (duration: 368.023419ms)","trace[2043981207] 'applied index is now lower than readState.Index'  (duration: 225.244¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-04-15T16:45:31.262237Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"368.711554ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-15T16:45:31.262276Z","caller":"traceutil/trace.go:171","msg":"trace[608485832] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:391020; }","duration":"368.800053ms","start":"2024-04-15T16:45:30.893466Z","end":"2024-04-15T16:45:31.262266Z","steps":["trace[608485832] 'agreement among raft nodes before linearized reading'  (duration: 368.668272ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:45:31.262321Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:45:30.893442Z","time spent":"368.863934ms","remote":"127.0.0.1:47248","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-04-15T16:45:31.262087Z","caller":"traceutil/trace.go:171","msg":"trace[2063034026] transaction","detail":"{read_only:false; response_revision:391020; number_of_response:1; }","duration":"655.61884ms","start":"2024-04-15T16:45:30.606445Z","end":"2024-04-15T16:45:31.262064Z","steps":["trace[2063034026] 'process raft request'  (duration: 655.050512ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-15T16:45:31.266601Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-15T16:45:30.606406Z","time spent":"659.860675ms","remote":"127.0.0.1:47506","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:391012 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}

* 
* ==> etcd [57977577d42e] <==
* {"level":"info","ts":"2024-04-17T16:47:33.66918Z","caller":"traceutil/trace.go:171","msg":"trace[2018757708] linearizableReadLoop","detail":"{readStateIndex:516981; appliedIndex:516980; }","duration":"336.198138ms","start":"2024-04-17T12:10:30.792184Z","end":"2024-04-17T16:47:33.666586Z","steps":["trace[2018757708] 'read index received'  (duration: 247.98261ms)","trace[2018757708] 'applied index is now lower than readState.Index'  (duration: 88.213254ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-17T16:47:33.669655Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"332.919373ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-17T16:47:33.66971Z","caller":"traceutil/trace.go:171","msg":"trace[1389120129] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:412545; }","duration":"333.057144ms","start":"2024-04-17T12:10:30.798432Z","end":"2024-04-17T16:47:33.669693Z","steps":["trace[1389120129] 'agreement among raft nodes before linearized reading'  (duration: 332.858326ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-17T16:47:33.670495Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-17T12:10:30.798387Z","time spent":"333.184486ms","remote":"127.0.0.1:54092","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-04-17T16:47:33.734783Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"336.609215ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-17T16:47:33.734954Z","caller":"traceutil/trace.go:171","msg":"trace[605849651] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:412545; }","duration":"404.595825ms","start":"2024-04-17T12:10:30.792122Z","end":"2024-04-17T16:47:33.734922Z","steps":["trace[605849651] 'agreement among raft nodes before linearized reading'  (duration: 336.567997ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-17T16:47:33.735038Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-17T12:10:30.792095Z","time spent":"404.698069ms","remote":"127.0.0.1:54316","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":30,"request content":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true "}
{"level":"warn","ts":"2024-04-17T16:47:33.8625Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-17T12:10:29.793378Z","time spent":"1.335326315s","remote":"127.0.0.1:54278","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:412544 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-17T16:49:06.956637Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":412382}
{"level":"info","ts":"2024-04-17T16:49:06.960349Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":412382,"took":"2.904484ms","hash":3216301688}
{"level":"info","ts":"2024-04-17T16:49:06.960484Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3216301688,"revision":412382,"compact-revision":412143}
{"level":"info","ts":"2024-04-17T16:49:19.231906Z","caller":"traceutil/trace.go:171","msg":"trace[421783569] transaction","detail":"{read_only:false; response_revision:412632; number_of_response:1; }","duration":"191.99847ms","start":"2024-04-17T16:49:19.037625Z","end":"2024-04-17T16:49:19.229624Z","steps":["trace[421783569] 'process raft request'  (duration: 191.736123ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T16:52:48.036675Z","caller":"traceutil/trace.go:171","msg":"trace[617226528] transaction","detail":"{read_only:false; response_revision:412801; number_of_response:1; }","duration":"104.64476ms","start":"2024-04-17T16:52:47.932004Z","end":"2024-04-17T16:52:48.036649Z","steps":["trace[617226528] 'process raft request'  (duration: 104.458767ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T16:54:06.973312Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":412623}
{"level":"info","ts":"2024-04-17T16:54:06.974993Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":412623,"took":"1.298034ms","hash":466957477}
{"level":"info","ts":"2024-04-17T16:54:06.975057Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":466957477,"revision":412623,"compact-revision":412382}
{"level":"info","ts":"2024-04-17T16:56:06.547584Z","caller":"traceutil/trace.go:171","msg":"trace[255569816] transaction","detail":"{read_only:false; response_revision:412961; number_of_response:1; }","duration":"110.045162ms","start":"2024-04-17T16:56:06.437505Z","end":"2024-04-17T16:56:06.547551Z","steps":["trace[255569816] 'process raft request'  (duration: 109.825354ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T16:57:29.546572Z","caller":"traceutil/trace.go:171","msg":"trace[356796114] transaction","detail":"{read_only:false; response_revision:413028; number_of_response:1; }","duration":"102.448117ms","start":"2024-04-17T16:57:29.444084Z","end":"2024-04-17T16:57:29.546532Z","steps":["trace[356796114] 'process raft request'  (duration: 85.956049ms)","trace[356796114] 'compare'  (duration: 16.322525ms)"],"step_count":2}
{"level":"info","ts":"2024-04-17T16:58:14.14694Z","caller":"traceutil/trace.go:171","msg":"trace[2095238341] transaction","detail":"{read_only:false; response_revision:413064; number_of_response:1; }","duration":"101.499581ms","start":"2024-04-17T16:58:14.04541Z","end":"2024-04-17T16:58:14.14691Z","steps":["trace[2095238341] 'process raft request'  (duration: 99.548782ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T16:59:06.980077Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":412865}
{"level":"info","ts":"2024-04-17T16:59:06.981541Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":412865,"took":"1.025033ms","hash":684953929}
{"level":"info","ts":"2024-04-17T16:59:06.981604Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":684953929,"revision":412865,"compact-revision":412623}
{"level":"info","ts":"2024-04-17T16:59:44.743211Z","caller":"traceutil/trace.go:171","msg":"trace[955705202] transaction","detail":"{read_only:false; response_revision:413136; number_of_response:1; }","duration":"110.766978ms","start":"2024-04-17T16:59:44.632417Z","end":"2024-04-17T16:59:44.743184Z","steps":["trace[955705202] 'process raft request'  (duration: 110.589931ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-17T17:02:21.343671Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.643278ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-17T17:02:21.343863Z","caller":"traceutil/trace.go:171","msg":"trace[2068330313] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:413260; }","duration":"101.193989ms","start":"2024-04-17T17:02:21.242645Z","end":"2024-04-17T17:02:21.343839Z","steps":["trace[2068330313] 'agreement among raft nodes before linearized reading'  (duration: 100.588875ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:02:21.343063Z","caller":"traceutil/trace.go:171","msg":"trace[790932613] transaction","detail":"{read_only:false; response_revision:413260; number_of_response:1; }","duration":"105.54271ms","start":"2024-04-17T17:02:21.237487Z","end":"2024-04-17T17:02:21.34303Z","steps":["trace[790932613] 'process raft request'  (duration: 105.098455ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:02:21.343028Z","caller":"traceutil/trace.go:171","msg":"trace[1640187683] linearizableReadLoop","detail":"{readStateIndex:517879; appliedIndex:517878; }","duration":"100.110275ms","start":"2024-04-17T17:02:21.242723Z","end":"2024-04-17T17:02:21.342833Z","steps":["trace[1640187683] 'read index received'  (duration: 99.77474ms)","trace[1640187683] 'applied index is now lower than readState.Index'  (duration: 333.311¬µs)"],"step_count":2}
{"level":"info","ts":"2024-04-17T17:04:06.989244Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":413106}
{"level":"info","ts":"2024-04-17T17:04:06.991403Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":413106,"took":"1.230212ms","hash":1688331968}
{"level":"info","ts":"2024-04-17T17:04:06.991488Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1688331968,"revision":413106,"compact-revision":412865}
{"level":"info","ts":"2024-04-17T17:04:16.842341Z","caller":"traceutil/trace.go:171","msg":"trace[767192870] transaction","detail":"{read_only:false; response_revision:413352; number_of_response:1; }","duration":"104.92857ms","start":"2024-04-17T17:04:16.737352Z","end":"2024-04-17T17:04:16.842281Z","steps":["trace[767192870] 'process raft request'  (duration: 104.622769ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:04:27.140021Z","caller":"traceutil/trace.go:171","msg":"trace[317462065] transaction","detail":"{read_only:false; response_revision:413360; number_of_response:1; }","duration":"101.723597ms","start":"2024-04-17T17:04:27.038259Z","end":"2024-04-17T17:04:27.139982Z","steps":["trace[317462065] 'process raft request'  (duration: 101.475334ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:09:07.008509Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":413344}
{"level":"info","ts":"2024-04-17T17:09:07.00988Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":413344,"took":"1.020399ms","hash":142666841}
{"level":"info","ts":"2024-04-17T17:09:07.009945Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":142666841,"revision":413344,"compact-revision":413106}
{"level":"info","ts":"2024-04-17T17:13:25.013081Z","caller":"traceutil/trace.go:171","msg":"trace[1404951054] transaction","detail":"{read_only:false; response_revision:413789; number_of_response:1; }","duration":"122.326362ms","start":"2024-04-17T17:13:24.890735Z","end":"2024-04-17T17:13:25.013061Z","steps":["trace[1404951054] 'process raft request'  (duration: 122.202106ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:14:07.035183Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":413584}
{"level":"info","ts":"2024-04-17T17:14:07.037047Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":413584,"took":"729.895¬µs","hash":3583696783}
{"level":"info","ts":"2024-04-17T17:14:07.037085Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3583696783,"revision":413584,"compact-revision":413344}
{"level":"info","ts":"2024-04-17T17:19:07.051362Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":413822}
{"level":"info","ts":"2024-04-17T17:19:07.05246Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":413822,"took":"845.503¬µs","hash":2775137460}
{"level":"info","ts":"2024-04-17T17:19:07.052513Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2775137460,"revision":413822,"compact-revision":413584}
{"level":"info","ts":"2024-04-17T17:24:07.058468Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":414062}
{"level":"info","ts":"2024-04-17T17:24:07.059306Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":414062,"took":"557.299¬µs","hash":3752454321}
{"level":"info","ts":"2024-04-17T17:24:07.059338Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3752454321,"revision":414062,"compact-revision":413822}
{"level":"info","ts":"2024-04-17T17:29:07.102756Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":414301}
{"level":"info","ts":"2024-04-17T17:29:07.103673Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":414301,"took":"605.969¬µs","hash":402928340}
{"level":"info","ts":"2024-04-17T17:29:07.103706Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":402928340,"revision":414301,"compact-revision":414062}
{"level":"info","ts":"2024-04-17T17:34:07.127645Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":414540}
{"level":"info","ts":"2024-04-17T17:34:07.128842Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":414540,"took":"907.004¬µs","hash":2495978514}
{"level":"info","ts":"2024-04-17T17:34:07.128879Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2495978514,"revision":414540,"compact-revision":414301}
{"level":"warn","ts":"2024-04-17T17:37:14.344431Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.690812ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-17T17:37:14.345945Z","caller":"traceutil/trace.go:171","msg":"trace[775783746] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:414929; }","duration":"105.393186ms","start":"2024-04-17T17:37:14.239263Z","end":"2024-04-17T17:37:14.344656Z","steps":["trace[775783746] 'range keys from in-memory index tree'  (duration: 100.414582ms)"],"step_count":1}
{"level":"info","ts":"2024-04-17T17:38:34.269317Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":520052,"local-member-snapshot-index":510051,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-04-17T17:38:34.289269Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":520052}
{"level":"info","ts":"2024-04-17T17:38:34.289441Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":515052}
{"level":"info","ts":"2024-04-17T17:38:37.074526Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000018-0000000000072c1f.snap"}
{"level":"info","ts":"2024-04-17T17:39:07.144942Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":414780}
{"level":"info","ts":"2024-04-17T17:39:07.147716Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":414780,"took":"2.448919ms","hash":3449860016}
{"level":"info","ts":"2024-04-17T17:39:07.147765Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3449860016,"revision":414780,"compact-revision":414540}

* 
* ==> kernel <==
*  17:40:10 up  8:07,  0 users,  load average: 0.12, 0.29, 0.34
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [51cdadf5ade1] <==
* I0417 16:51:06.638761       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:52:06.638102       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:53:06.639629       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:54:06.640627       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:54:09.077251       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:55:06.637944       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:56:06.640397       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:57:06.637559       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:58:06.638341       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:59:06.637424       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 16:59:09.084339       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:00:06.636762       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:01:06.638269       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:02:06.638502       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:03:06.637488       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:04:06.638404       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:04:09.092373       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:05:06.637808       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:06:06.637049       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:07:06.637655       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:08:06.644892       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:09:06.636805       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:09:09.097237       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:10:06.636229       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:11:06.633503       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:12:06.634442       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:13:06.633079       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:14:06.633598       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:14:09.097138       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:15:06.637285       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:16:06.634403       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:17:06.633368       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:18:06.636379       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:19:06.638752       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:19:09.108077       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:20:06.639733       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:21:06.639393       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:22:06.638195       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:23:06.638543       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:24:06.638352       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:24:09.118440       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:25:06.638780       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:26:06.639548       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:27:06.639566       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:28:06.639395       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:29:06.638463       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:29:09.137572       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:30:06.638911       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:31:06.639084       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:32:06.637787       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:33:06.638539       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:34:06.638293       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:34:09.145189       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:35:06.638676       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:36:06.637513       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:37:06.639100       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:38:06.638280       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:39:06.639338       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:39:09.150573       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0417 17:40:06.638918       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-apiserver [a3d14ef9360b] <==
* I0415 16:28:46.526600       1 controller.go:624] quota admission added evaluator for: endpoints
I0415 16:28:46.542454       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0415 16:28:46.831135       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0415 16:28:47.038121       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io "v1beta1.metrics.k8s.io": the object has been modified; please apply your changes to the latest version and try again
I0415 16:28:47.126966       1 trace.go:236] Trace[1451373203]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:9a119d3f-caa3-47e9-939a-6b3d0c1802b9,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/services/dashboard-metrics-scraper,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (15-Apr-2024 16:28:46.544) (total time: 581ms):
Trace[1451373203]: ---"About to write a response" 580ms (16:28:47.125)
Trace[1451373203]: [581.097237ms] [581.097237ms] END
I0415 16:28:47.126970       1 trace.go:236] Trace[1817603649]: "Get" accept:application/vnd.kubernetes.protobuf;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json;as=PartialObjectMetadata;g=meta.k8s.io;v=v1,application/json,audit-id:16621728-e3f3-4ac9-b491-c813165afd19,client:192.168.49.2,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/services/kubernetes-dashboard,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:generic-garbage-collector,verb:GET (15-Apr-2024 16:28:46.542) (total time: 584ms):
Trace[1817603649]: ---"About to write a response" 583ms (16:28:47.125)
Trace[1817603649]: [584.029632ms] [584.029632ms] END
W0415 16:28:47.135499       1 handler_proxy.go:93] no RequestInfo found in the context
E0415 16:28:47.135620       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0415 16:28:47.728937       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0415 16:28:48.135810       1 handler_proxy.go:93] no RequestInfo found in the context
E0415 16:28:48.135956       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0415 16:28:48.135974       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0415 16:28:48.136055       1 handler_proxy.go:93] no RequestInfo found in the context
E0415 16:28:48.136085       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0415 16:28:48.224234       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0415 16:28:49.934385       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:28:49.935935       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0415 16:28:50.926676       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0415 16:28:51.324476       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0415 16:28:51.927858       1 aggregator.go:166] failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0415 16:28:54.341781       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.96.213.58:443/apis/metrics.k8s.io/v1beta1: Get "https://10.96.213.58:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.96.213.58:443: connect: connection refused
W0415 16:28:54.344141       1 handler_proxy.go:93] no RequestInfo found in the context
E0415 16:28:54.344303       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0415 16:28:54.536931       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:28:54.634900       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:29:23.062044       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:30:23.062488       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:31:23.061591       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:32:23.062334       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:33:23.058102       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:33:23.534873       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:34:23.054752       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:35:23.055975       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:36:23.057583       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:37:23.128438       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:38:23.057189       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:38:23.541318       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:39:23.056004       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:40:23.053661       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:41:23.053623       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:42:23.052965       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:43:23.051209       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:43:23.541501       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:44:23.051629       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:45:28.114898       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0415 16:45:31.267532       1 trace.go:236] Trace[563336522]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:b7b69d1d-ed35-4133-90c9-2d9043e3bbd2,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (15-Apr-2024 16:45:30.603) (total time: 664ms):
Trace[563336522]: ["GuaranteedUpdate etcd3" audit-id:b7b69d1d-ed35-4133-90c9-2d9043e3bbd2,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 663ms (16:45:30.603)
Trace[563336522]:  ---"Txn call completed" 662ms (16:45:31.267)]
Trace[563336522]: [664.027167ms] [664.027167ms] END

* 
* ==> kube-controller-manager [736797f5de2d] <==
* I0417 11:13:58.511862       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="80.984653ms"
I0417 11:13:58.512139       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="113.225¬µs"
I0417 11:13:58.611764       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="80.451305ms"
I0417 11:13:58.611958       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="70.634¬µs"
I0417 11:14:08.297979       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E0417 11:14:08.306275       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I0417 11:14:08.306806       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0417 11:14:23.334939       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:14:23.352281       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:14:23.352452       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:14:38.365452       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:14:38.365548       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:14:38.366141       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:14:53.385742       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:14:53.385901       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:14:53.386243       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:15:08.397923       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:15:08.398001       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:15:08.398160       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:15:14.055659       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="105.921¬µs"
I0417 11:15:23.402908       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"frontend\" not found"
E0417 11:15:23.422318       1 horizontal.go:274] failed to query scale subresource for Deployment/default/frontend: deployments/scale.apps "frontend" not found
E0417 11:15:38.427048       1 horizontal.go:274] failed to query scale subresource for Deployment/default/frontend: deployments/scale.apps "frontend" not found
I0417 11:15:38.427091       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"frontend\" not found"
E0417 11:15:53.438059       1 horizontal.go:274] failed to query scale subresource for Deployment/default/frontend: deployments/scale.apps "frontend" not found
I0417 11:15:53.438319       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"frontend\" not found"
I0417 11:15:56.803219       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set frontend-795b566649 to 3"
I0417 11:15:56.826330       1 event.go:307] "Event occurred" object="default/frontend-795b566649" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-795b566649-447g5"
I0417 11:15:56.834170       1 event.go:307] "Event occurred" object="default/frontend-795b566649" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-795b566649-ndxnf"
I0417 11:15:56.841731       1 event.go:307] "Event occurred" object="default/frontend-795b566649" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-795b566649-cxnbl"
I0417 11:15:56.858928       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="55.732912ms"
I0417 11:15:56.913620       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="54.548544ms"
I0417 11:15:56.945424       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="31.667297ms"
I0417 11:15:56.945668       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="117.623¬µs"
I0417 11:15:56.959615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="124.186¬µs"
I0417 11:15:59.826582       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="78.244664ms"
I0417 11:15:59.827060       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="148.881¬µs"
I0417 11:15:59.922879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="85.556437ms"
I0417 11:15:59.924470       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="125.638¬µs"
I0417 11:15:59.946010       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="15.084907ms"
I0417 11:15:59.947186       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="86.544¬µs"
I0417 11:16:08.453352       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E0417 11:16:08.485301       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I0417 11:16:08.485399       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0417 11:16:23.506282       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:16:23.514864       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:16:23.514995       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:16:38.526888       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:16:38.527076       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:16:38.527117       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:16:53.543847       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:16:53.543970       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:16:53.544170       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
E0417 11:17:08.557051       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/frontend: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)
I0417 11:17:08.557240       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 11:17:08.557280       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for targeted pods (pods might be unready)"
I0417 16:47:45.490009       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="Unauthorized"
E0417 16:47:45.508622       1 horizontal.go:274] failed to query scale subresource for Deployment/default/frontend: Unauthorized
E0417 16:47:58.571257       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0417 16:47:58.880024       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-controller-manager [a33ec903e708] <==
* I0415 16:28:45.433541       1 shared_informer.go:318] Caches are synced for service account
I0415 16:28:45.433981       1 shared_informer.go:318] Caches are synced for TTL after finished
I0415 16:28:45.436123       1 shared_informer.go:318] Caches are synced for TTL
I0415 16:28:45.436155       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0415 16:28:45.436215       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0415 16:28:45.438117       1 range_allocator.go:174] "Sending events to api server"
I0415 16:28:45.438223       1 range_allocator.go:178] "Starting range CIDR allocator"
I0415 16:28:45.438240       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0415 16:28:45.438254       1 shared_informer.go:318] Caches are synced for cidrallocator
I0415 16:28:45.438609       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0415 16:28:45.438660       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0415 16:28:45.524577       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0415 16:28:45.436440       1 shared_informer.go:318] Caches are synced for namespace
I0415 16:28:45.730751       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0415 16:28:45.734871       1 shared_informer.go:318] Caches are synced for cronjob
I0415 16:28:45.736840       1 trace.go:236] Trace[1987969734]: "DeltaFIFO Pop Process" ID:system:kube-controller-manager,Depth:14,Reason:slow event handlers blocking the queue (15-Apr-2024 16:28:45.636) (total time: 100ms):
Trace[1987969734]: [100.156869ms] [100.156869ms] END
I0415 16:28:45.835373       1 shared_informer.go:318] Caches are synced for expand
I0415 16:28:45.837217       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0415 16:28:45.924673       1 shared_informer.go:318] Caches are synced for ephemeral
I0415 16:28:45.924676       1 shared_informer.go:318] Caches are synced for GC
I0415 16:28:45.924777       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0415 16:28:45.924831       1 shared_informer.go:318] Caches are synced for resource quota
I0415 16:28:45.927880       1 shared_informer.go:318] Caches are synced for resource quota
I0415 16:28:45.928008       1 shared_informer.go:318] Caches are synced for daemon sets
I0415 16:28:45.929750       1 shared_informer.go:318] Caches are synced for attach detach
I0415 16:28:45.931094       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-795b566649" duration="6.028282ms"
I0415 16:28:45.931429       1 shared_informer.go:318] Caches are synced for deployment
I0415 16:28:45.931812       1 shared_informer.go:318] Caches are synced for ReplicationController
I0415 16:28:45.931841       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-78f87ddfc" duration="676.651¬µs"
I0415 16:28:45.932240       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="286.847¬µs"
I0415 16:28:45.932287       1 shared_informer.go:318] Caches are synced for job
I0415 16:28:45.932981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5657497c4c" duration="4.318497ms"
I0415 16:28:45.937507       1 shared_informer.go:318] Caches are synced for HPA
I0415 16:28:45.940132       1 shared_informer.go:318] Caches are synced for stateful set
I0415 16:28:46.027189       1 shared_informer.go:318] Caches are synced for disruption
I0415 16:28:46.027391       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="99.968809ms"
I0415 16:28:46.027497       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0415 16:28:46.027677       1 shared_informer.go:318] Caches are synced for endpoint
I0415 16:28:46.031956       1 shared_informer.go:318] Caches are synced for persistent volume
I0415 16:28:46.033125       1 shared_informer.go:318] Caches are synced for PVC protection
I0415 16:28:46.033161       1 shared_informer.go:318] Caches are synced for taint
I0415 16:28:46.131069       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0415 16:28:46.136833       1 taint_manager.go:211] "Sending events to api server"
I0415 16:28:46.228305       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0415 16:28:46.229783       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0415 16:28:46.236621       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0415 16:28:46.240549       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0415 16:28:46.328629       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0415 16:28:46.529041       1 shared_informer.go:318] Caches are synced for garbage collector
I0415 16:28:46.535966       1 shared_informer.go:318] Caches are synced for garbage collector
I0415 16:28:46.536120       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0415 16:28:47.031551       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.106620919s"
I0415 16:28:47.031747       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="86.914¬µs"
I0415 16:28:47.037764       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="1.11142989s"
I0415 16:28:47.043398       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="201.474¬µs"
I0415 16:28:49.727523       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="187.971705ms"
I0415 16:28:49.732197       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="213.979¬µs"
I0415 16:28:54.330129       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="84.033376ms"
I0415 16:28:54.330314       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="75.836¬µs"

* 
* ==> kube-proxy [669400216a1e] <==
* I0415 16:28:38.540900       1 server_others.go:69] "Using iptables proxy"
I0415 16:28:38.835284       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0415 16:28:39.535177       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0415 16:28:39.543195       1 server_others.go:152] "Using iptables Proxier"
I0415 16:28:39.543364       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0415 16:28:39.543409       1 server_others.go:438] "Defaulting to no-op detect-local"
I0415 16:28:39.629943       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0415 16:28:39.633508       1 server.go:846] "Version info" version="v1.28.3"
I0415 16:28:39.635413       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0415 16:28:39.730953       1 config.go:188] "Starting service config controller"
I0415 16:28:39.731212       1 config.go:97] "Starting endpoint slice config controller"
I0415 16:28:39.732300       1 shared_informer.go:311] Waiting for caches to sync for service config
I0415 16:28:39.732435       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0415 16:28:39.734460       1 config.go:315] "Starting node config controller"
I0415 16:28:39.734540       1 shared_informer.go:311] Waiting for caches to sync for node config
I0415 16:28:39.926444       1 shared_informer.go:318] Caches are synced for service config
I0415 16:28:39.933831       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0415 16:28:39.935095       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [c23261166b18] <==
* I0415 17:05:08.252468       1 server_others.go:69] "Using iptables proxy"
I0415 17:05:08.448970       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0415 17:05:08.645799       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0415 17:05:08.647790       1 server_others.go:152] "Using iptables Proxier"
I0415 17:05:08.647867       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0415 17:05:08.647876       1 server_others.go:438] "Defaulting to no-op detect-local"
I0415 17:05:08.649438       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0415 17:05:08.651010       1 server.go:846] "Version info" version="v1.28.3"
I0415 17:05:08.651056       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0415 17:05:08.654971       1 config.go:188] "Starting service config controller"
I0415 17:05:08.655172       1 config.go:97] "Starting endpoint slice config controller"
I0415 17:05:08.655297       1 config.go:315] "Starting node config controller"
I0415 17:05:08.656708       1 shared_informer.go:311] Waiting for caches to sync for node config
I0415 17:05:08.656712       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0415 17:05:08.656725       1 shared_informer.go:311] Waiting for caches to sync for service config
I0415 17:05:08.757555       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0415 17:05:08.757581       1 shared_informer.go:318] Caches are synced for node config
I0415 17:05:08.757555       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [010e3a144e40] <==
* I0415 17:05:01.575391       1 serving.go:348] Generated self-signed cert in-memory
W0415 17:05:04.532125       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0415 17:05:04.532180       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0415 17:05:04.532195       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0415 17:05:04.532206       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0415 17:05:04.553072       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0415 17:05:04.553108       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0415 17:05:04.555078       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0415 17:05:04.555596       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0415 17:05:04.555868       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0415 17:05:04.555955       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0415 17:05:04.655804       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [f11c9c7d3e23] <==
* I0415 16:28:18.474301       1 serving.go:348] Generated self-signed cert in-memory
I0415 16:28:23.827349       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0415 16:28:23.827431       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0415 16:28:24.032674       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0415 16:28:24.033589       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0415 16:28:24.033847       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0415 16:28:24.033914       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0415 16:28:24.034544       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0415 16:28:24.034564       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0415 16:28:24.034596       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0415 16:28:24.034669       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0415 16:28:24.133884       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0415 16:28:24.134187       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0415 16:28:24.134913       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Apr 17 17:35:11 minikube kubelet[2574]: E0417 17:35:11.798707    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:21 minikube kubelet[2574]: E0417 17:35:21.828268    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:21 minikube kubelet[2574]: E0417 17:35:21.828343    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:31 minikube kubelet[2574]: E0417 17:35:31.856736    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:31 minikube kubelet[2574]: E0417 17:35:31.856801    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:41 minikube kubelet[2574]: E0417 17:35:41.880835    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:41 minikube kubelet[2574]: E0417 17:35:41.880903    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:51 minikube kubelet[2574]: E0417 17:35:51.905485    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:35:51 minikube kubelet[2574]: E0417 17:35:51.905554    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:02 minikube kubelet[2574]: E0417 17:36:02.136801    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:02 minikube kubelet[2574]: E0417 17:36:02.136879    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:12 minikube kubelet[2574]: E0417 17:36:12.163263    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:12 minikube kubelet[2574]: E0417 17:36:12.163336    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:22 minikube kubelet[2574]: E0417 17:36:22.262071    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:22 minikube kubelet[2574]: E0417 17:36:22.262156    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:32 minikube kubelet[2574]: E0417 17:36:32.289595    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:32 minikube kubelet[2574]: E0417 17:36:32.289670    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:42 minikube kubelet[2574]: E0417 17:36:42.322865    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:42 minikube kubelet[2574]: E0417 17:36:42.322974    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:52 minikube kubelet[2574]: E0417 17:36:52.379127    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:36:52 minikube kubelet[2574]: E0417 17:36:52.379295    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:02 minikube kubelet[2574]: E0417 17:37:02.416058    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:02 minikube kubelet[2574]: E0417 17:37:02.416135    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:12 minikube kubelet[2574]: E0417 17:37:12.451575    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:12 minikube kubelet[2574]: E0417 17:37:12.452051    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:22 minikube kubelet[2574]: E0417 17:37:22.484477    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:22 minikube kubelet[2574]: E0417 17:37:22.484601    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:32 minikube kubelet[2574]: E0417 17:37:32.513579    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:32 minikube kubelet[2574]: E0417 17:37:32.513646    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:42 minikube kubelet[2574]: E0417 17:37:42.541701    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:42 minikube kubelet[2574]: E0417 17:37:42.541800    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:52 minikube kubelet[2574]: E0417 17:37:52.571346    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:37:52 minikube kubelet[2574]: E0417 17:37:52.571423    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:02 minikube kubelet[2574]: E0417 17:38:02.600184    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:02 minikube kubelet[2574]: E0417 17:38:02.600254    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:12 minikube kubelet[2574]: E0417 17:38:12.628019    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:12 minikube kubelet[2574]: E0417 17:38:12.628090    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:22 minikube kubelet[2574]: E0417 17:38:22.657848    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:22 minikube kubelet[2574]: E0417 17:38:22.657959    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:32 minikube kubelet[2574]: E0417 17:38:32.686660    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:32 minikube kubelet[2574]: E0417 17:38:32.686733    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:42 minikube kubelet[2574]: E0417 17:38:42.714472    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:42 minikube kubelet[2574]: E0417 17:38:42.714545    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:52 minikube kubelet[2574]: E0417 17:38:52.741589    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:38:52 minikube kubelet[2574]: E0417 17:38:52.741656    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:01 minikube kubelet[2574]: W0417 17:39:01.240987    2574 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 17 17:39:02 minikube kubelet[2574]: E0417 17:39:02.771837    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:02 minikube kubelet[2574]: E0417 17:39:02.771907    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:12 minikube kubelet[2574]: E0417 17:39:12.799313    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:12 minikube kubelet[2574]: E0417 17:39:12.799393    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:22 minikube kubelet[2574]: E0417 17:39:22.823558    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:22 minikube kubelet[2574]: E0417 17:39:22.823626    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:32 minikube kubelet[2574]: E0417 17:39:32.850982    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:32 minikube kubelet[2574]: E0417 17:39:32.851088    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:42 minikube kubelet[2574]: E0417 17:39:42.878304    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:42 minikube kubelet[2574]: E0417 17:39:42.878396    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:52 minikube kubelet[2574]: E0417 17:39:52.907115    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:39:52 minikube kubelet[2574]: E0417 17:39:52.907183    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:40:02 minikube kubelet[2574]: E0417 17:40:02.931349    2574 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"
Apr 17 17:40:02 minikube kubelet[2574]: E0417 17:40:02.931417    2574 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log\": failed to reopen container log \"1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_coredns-5dd5756b68-2wlqh_dc0090b5-b39e-43d8-a7ce-de18be7fc19a/coredns/23.log" containerID="1a4741fbfc142cd4ac14fca82651e766135fc701dc99ab72eb8b992fa2aeba1c"

* 
* ==> kubernetes-dashboard [2b390df4e33a] <==
* 2024/04/15 17:05:40 Using namespace: kubernetes-dashboard
2024/04/15 17:05:40 Using in-cluster config to connect to apiserver
2024/04/15 17:05:40 Using secret token for csrf signing
2024/04/15 17:05:40 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/04/15 17:05:40 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/04/15 17:05:40 Successful initial request to the apiserver, version: v1.28.3
2024/04/15 17:05:40 Generating JWE encryption key
2024/04/15 17:05:40 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/04/15 17:05:40 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/04/15 17:05:40 Initializing JWE encryption key from synchronized object
2024/04/15 17:05:40 Creating in-cluster Sidecar client
2024/04/15 17:05:40 Serving insecurely on HTTP port: 9090
2024/04/15 17:05:40 Successful request to sidecar
2024/04/15 17:05:40 Starting overwatch

* 
* ==> kubernetes-dashboard [7ac64c8d4051] <==
* 2024/04/15 17:05:08 Using namespace: kubernetes-dashboard
2024/04/15 17:05:08 Using in-cluster config to connect to apiserver
2024/04/15 17:05:08 Using secret token for csrf signing
2024/04/15 17:05:08 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/04/15 17:05:08 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00079fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000158100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [66d3adf39c8c] <==
* I0415 17:05:07.641914       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0415 17:05:17.652706       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [e75421dbc807] <==
* I0415 17:05:33.061599       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0415 17:05:33.074483       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0415 17:05:33.074879       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0415 17:05:50.472078       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0415 17:05:50.472309       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"88c12e45-e0c7-41b3-951a-e3e6f9abcd5f", APIVersion:"v1", ResourceVersion:"391214", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_22ba51a5-42be-4a78-86dc-50ffdf641413 became leader
I0415 17:05:50.472445       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_22ba51a5-42be-4a78-86dc-50ffdf641413!
I0415 17:05:50.573399       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_22ba51a5-42be-4a78-86dc-50ffdf641413!

